{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "-ufToSzcSuMI",
   "metadata": {
    "id": "-ufToSzcSuMI"
   },
   "source": [
    "# **EUROPEAN MONARCHIES NETWORK**\n",
    "Our final project consists of a study of the european royal families. We study the members of each family, how they are related to each other, and how they are related to other members of other families. The main pieces of interesting information would be discovering family relationships between different royal houses or countries, because different royal houses from different countries are related to each other due to historical reasons, for example marriages to form alliances or increase power.\n",
    "\n",
    "It would be interesting to be able to identify common ancestors of the actual Kings and Queens of Europe, like for example Queen Victoria of the United Kingdom.\n",
    "\n",
    "To do this, the objective is to create a network with the following characteristics:\n",
    "\n",
    "Nodes:\n",
    "\n",
    "\n",
    "*   All Kings, Queens, Spouses, and brothers (to expand the network, we can explore the cousins of the monarchs too)\n",
    "*   Node size based on importance (priority in the line of succession). Being monarchs the nodes with the biggest size.\n",
    "*   Circular nodes with the images of the character which that node represents.\n",
    "*   Border colour of the node based on the country or the royal household from the character that the node represents.\n",
    "\n",
    "Edges:\n",
    "\n",
    "\n",
    "*   They represent the relationship between two characters or nodes.\n",
    "*   We can have different colours for different types of relationships (Direct succession/predecession, husband or wife, brotherhood, sons which never accessed the throne)\n",
    "*   Different types of relationships can be also modeled using width of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p59G3EL5N4oS",
   "metadata": {
    "id": "p59G3EL5N4oS"
   },
   "source": [
    "# ***Important!!! ***\n",
    "\n",
    "If some issues occur in running the code, figures could be seen [here](https://nbviewer.org/github/Davide011/Social_Graph/blob/main/COLAB_FINAL_PROJECT.ipynb) .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c470843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "import os.path\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk import word_tokenize\n",
    "from nltk.text import TextCollection\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')   # you don't need to run this every time if you are not in colab\n",
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ewvvjw_dexZa",
   "metadata": {
    "id": "ewvvjw_dexZa"
   },
   "source": [
    "Load a full graph from a python .pkl file (if you run this, skip all the network construction part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LOCAL\n",
    "\n",
    "save_node_dir = r\"C:\\Users\\david\\Desktop\\CORSI ORAAA\\SOCIAL-GRAPH\\Final_project\\graph\"\n",
    "\n",
    "# chnage the name of the file to load here\n",
    "graph_file = save_node_dir + \"\\\\\" + \"graph_all_1450_new.pkl\"  #imbrscore\n",
    "#graph_file = save_node_dir + \"\\\\\" + \"graph_all_1450.pkl\"      # all !!\n",
    "\n",
    "# load the graph from a file\n",
    "with open(graph_file, 'rb') as file:\n",
    "    G = pickle.load(file)\n",
    "#G =  pickle.load(save_node_dir + \"\\\\\" + \"graph_1450.pkl\")\n",
    "#G = nx.read_gpickle(save_node_dir + \"\\\\\" + \"graph.gpickle\")\n",
    "\n",
    "#OK NOW YOU CAN SKIPP all the part below! :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "icTx-bdddmnY",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# load the graph from a file\n",
    "base_path = \"/content/drive/MyDrive/Graph_1450\"    # path davide\n",
    "#base_path = \"/content/drive/MyDrive\"    # path sergi\n",
    "#base_path = \"/content/drive/MyDrive/DTU/Social Graphs 2023/FINAL VERSION/pkl files here (put in your drive to to load in colab!!)\"    # path pere\n",
    "#base_path = './' # Pere local\n",
    "\n",
    "with open(base_path + \"/\" + \"graph_all_1450_new.pkl\", 'rb') as file:\n",
    "    G = pickle.load(file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pEW8dMw8Mjrl",
   "metadata": {
    "heading_collapsed": true,
    "id": "pEW8dMw8Mjrl"
   },
   "source": [
    "# NETWORK CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7ab66",
   "metadata": {
    "hidden": true,
    "id": "a5b7ab66"
   },
   "source": [
    "We will start from a list of the 18 last/latest monarchs in different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Given data\n",
    "data = {\n",
    "    'country': ['Belgium', 'Bulgaria', 'Denmark', 'Greece', 'Italy', 'Luxembourg', 'Montenegro',\n",
    "                'Netherlands', 'Norway', 'Portugal', 'Romania', 'Russia', 'Serbia', 'Spain', 'Sweden', 'UK', 'Liechtenstein', 'Monaco'],\n",
    "\n",
    "    'monarch_name': ['Philippe of Belgium', 'Simeon Saxe-Coburg-Gotha', 'Margrethe II of Denmark', 'Constantine II of Greece',\n",
    "                     'Umberto II of Italy', 'Henri, Grand Duke of Luxembourg', 'Nicholas I of Montenegro', 'Willem-Alexander of the Netherlands',\n",
    "                     'Harald V of Norway', 'King Manuel II', 'Michael I of Romania', 'Nicholas II of Russia', 'Peter I of Serbia',\n",
    "                     'Felipe VI', 'Carl XVI Gustaf', 'Charles III', 'Hans-Adam II, Prince of Liechtenstein', 'Albert II, Prince of Monaco'],\n",
    "\n",
    "    'time': ['2013 - Present', 'Ended in 1946', '1972 - Present', 'Ended in 1973', 'Effectively ended in 1946',\n",
    "             '2000 - Present', 'Ended in 1918', '2013 - Present', '1991 - Present', 'Ended in 1910',\n",
    "             '1927-1930, 1930-1937', 'Ended in 1917', 'Ended in 1921', '2014 - Present', '1973 - Present', '1952 - Present', '2004 - Present', '2005 - Present']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Creating a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Adding a new column with monarch names spaced by underscores\n",
    "df['monarch_name_spaced'] = df['monarch_name'].str.replace(' ', '_')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddde6f",
   "metadata": {
    "hidden": true,
    "id": "6bddde6f"
   },
   "source": [
    "Many Wikipedia articles have multiple links that redirect to them. If we want to get the correct data for nodes, we have to get the original article that a name/string links to. We can do this via API calls, but that is very costly for graphs with a lot of node, so Instead we've built a link-article dictionary that stores all the redirects we've come across. It can be loaded from a pickle (.pkl) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba097172",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/Graph_1450\"    # path davide\n",
    "#base_path = \"/content/drive/MyDrive\"\n",
    "#base_path = \"/content/drive/MyDrive/DTU/Social Graphs 2023\" # path pere\n",
    "\n",
    "all_redirects = {}\n",
    "#with open( os.path.join(base_path,'redirects_1450_.pkl'),'rb') as f:\n",
    "with open( os.path.join(base_path,'all_redirects_plus.pkl'),'rb') as f:\n",
    "    all_redirects = pickle.load(f)\n",
    "\n",
    "# People who redirect to other people...\n",
    "all_redirects['Princess Margarete Karola of Saxony'] = None\n",
    "all_redirects[' Elena María de Castellví y Shelly'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9836ff2",
   "metadata": {
    "hidden": true,
    "id": "b9836ff2"
   },
   "source": [
    "Here we define auxiliary functions to deal with wikipedia information: getting wikipedia articles from their links, links from text, redirects from links..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab835f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(line):\n",
    "    \"\"\"Returns all Wikipedia links in a given string/line, use with caution to not get unwanted links\"\"\"\n",
    "    #built specifically for malfunctioning \"issue\" attributes\n",
    "    links = []\n",
    "    # lsearch is a list of all the link objects in the line\n",
    "    lsearch = re.findall( r'\\[\\[(.*?)\\]\\]', line)\n",
    "    for longlink in lsearch:\n",
    "        # Leaves only the link name from each of the link objects found\n",
    "        link = longlink.split('|')[0].replace('[','').replace(']','').replace('&nbsp;',' ').replace('_',' ')\n",
    "        link = get_redirect(link)\n",
    "        if link is not None:\n",
    "          links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "def get_link(line, keyword=None):\n",
    "    \"\"\"\n",
    "    Returns the first link in a string if keyword isn't passed.\n",
    "        Otherwise, returns the last link before the keyword\n",
    "    \"\"\"\n",
    "    # Extract wikipedia page link name from a line of text\n",
    "    lsearch = re.findall( r'\\[\\[(.*?)\\]\\]', line)\n",
    "    idx = 0\n",
    "    # A bit hacky, finds the last link before a keyword. If keyword not present, will return the last link\n",
    "    # Right now it only serves to get the correct predecessor (if there are multiple, return the king for example)\n",
    "    if len(lsearch)>1 and keyword is not None and keyword.lower() in line.lower():\n",
    "        for idx, el in enumerate(lsearch):\n",
    "            spl = line.lower().split(el.lower())\n",
    "            if keyword not in spl[1]:\n",
    "                idx -= 1 if idx > 0 else 0\n",
    "                break\n",
    "    #if len(lsearch)>1 and keyword is not None: print(lsearch)\n",
    "    longlink = lsearch[idx] if len(lsearch)>0 else None\n",
    "    link = longlink.split('|')[0].replace('[','').replace(']','').replace('&nbsp;',' ').replace('_',' ') if longlink is not None else None\n",
    "\n",
    "    # Gives us the link of the original article (if link is a redirect)\n",
    "    link = get_redirect(link)\n",
    "\n",
    "    return link\n",
    "\n",
    "def get_wikitext(page_link):\n",
    "    \"\"\"Returns the wikitext from the link passed (or its redirect)\"\"\"\n",
    "    page_link = get_redirect(page_link)\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": page_link\n",
    "    }\n",
    "    wikijson = requests.get(baseurl, params=params).json()\n",
    "    if 'query' not in wikijson.keys(): return None, page_link\n",
    "    if 'pages' not in wikijson['query'].keys(): return None, page_link\n",
    "    numkey = list(wikijson['query']['pages'].keys())[0]\n",
    "    # Check if the wikipedia article actually exists for the link\n",
    "    if 'revisions' not in wikijson['query']['pages'][numkey].keys(): return None, page_link\n",
    "    wikitext = wikijson['query']['pages'][numkey]['revisions'][0]['*']\n",
    "\n",
    "    # Some links in pages are redirects :/ (shouldn't be needed thanks to get_redirect)\n",
    "    if wikitext is not None and wikitext.lower().startswith('#redirect'):\n",
    "        page_link = get_link(wikitext)\n",
    "        wikitext, _ = get_wikitext(page_link)\n",
    "\n",
    "    return wikitext, page_link\n",
    "\n",
    "def get_redirect(link):\n",
    "    \"\"\"Returns the article link the passed link redirects to if there is one\"\"\"\n",
    "    if link is not None:\n",
    "        link = link.replace('_',' ')\n",
    "        #if link.lower().startswith('house') or 'family' in link.lower(): return None\n",
    "        if link in all_redirects.keys(): return all_redirects[link]\n",
    "    # Queries the wikipedia API to find a link's redirect\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?redirects&\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": link\n",
    "    }\n",
    "    wikijson = requests.get(baseurl, params=params).json()\n",
    "    if 'query' not in wikijson.keys(): return link\n",
    "    if 'redirects' in wikijson['query'].keys():\n",
    "        new_link = wikijson['query']['redirects'][0]['to']\n",
    "        all_redirects[link]=new_link\n",
    "        link = new_link\n",
    "    else:\n",
    "        all_redirects[link]=link\n",
    "    #if link.lower().startswith('house') or 'family' in link.lower(): return None\n",
    "    return link.replace('_',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d34bc",
   "metadata": {
    "hidden": true,
    "id": "bc2d34bc"
   },
   "source": [
    "Function to build a node from a given article name/link. Returns a node item as well as its neighbors in the network. The built nodes will have some attributes from their wikipedia article:\n",
    "- monarch: wether the person is reigning or has reigned over something in the past.\n",
    "- house: the royal family the person belongs to\n",
    "- reign_start: the start of the person's reign, if it has one\n",
    "- birth/death: the person's date of birth and death (if present)\n",
    "- person_type: the type that wikipedia gives to this person's infobox's style. This is not very reliable\n",
    "- parents: the person's parents\n",
    "\n",
    "Appart from this, the function will return the person's parents, children and predecessor (if the person has one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_has_attr(attr, line):\n",
    "    # Check a line from a Wikipedia article for a certain infobox attribute\n",
    "    return re.match(r'\\|\\s*'+attr, line) is not None\n",
    "\n",
    "def get_node(name):\n",
    "    \"\"\"Returns a node and neighbors for a link/person name\"\"\"\n",
    "    # Get a node from a wikipedia link name\n",
    "    try:\n",
    "        text, name = get_wikitext(name)\n",
    "    except OSError as e:\n",
    "        print(e,name,'failed us')\n",
    "    if text is None: return None, None, None, None, None\n",
    "\n",
    "    # Initialize all items to None or empty, in the case that they are not present\n",
    "    predecessor = None\n",
    "    spouses = []\n",
    "    house = None\n",
    "    reign_start = None\n",
    "    birth = None\n",
    "    death = None\n",
    "    offspring = []\n",
    "    parents = []\n",
    "    # Used to check if a list of spouses or children has started\n",
    "    found_issue = False\n",
    "    found_spouses = False\n",
    "    has_reign = False\n",
    "    person_type = None\n",
    "    # Parse the text line by line, as we'll use the infobox of people's pages\n",
    "    for line in text.splitlines():\n",
    "        # '*' represent list items. In this case useful for lists of children/spouses\n",
    "        if line.startswith('*'):\n",
    "            if found_issue:\n",
    "                if get_link(line) is not None:\n",
    "                    offspring.append(get_link(line))\n",
    "            elif found_spouses:\n",
    "                if get_link(line) is not None:\n",
    "                    spouses.append(get_link(line))\n",
    "        else:\n",
    "            #when we reach a non-* character after a list of offspring/spouses, it means it has ended\n",
    "            found_issue = False\n",
    "            found_spouses = False\n",
    "            # All items in the person's infobox start with a vertical line\n",
    "            if line.startswith('|'):\n",
    "                # Give priority to predecessor without a number after over predecessorX or regent\n",
    "                if line_has_attr('predecessor ', line):\n",
    "                    predecessor = get_link(line, 'king')\n",
    "                elif line_has_attr('predecessor', line) and predecessor is None:\n",
    "                    predecessor = get_link(line, 'king')\n",
    "                elif line_has_attr('father ', line) and get_link(line) is not None:\n",
    "                    parents.append(get_link(line))\n",
    "                elif line_has_attr('mother ', line) and get_link(line) is not None:\n",
    "                    parents.append(get_link(line))\n",
    "                elif line_has_attr('spouse ', line) and len(spouses)==0:\n",
    "                    if get_link(line) is not None:\n",
    "                        spouses.append(get_link(line))\n",
    "                elif line_has_attr('spouses ', line):\n",
    "                    found_spouses = True\n",
    "                elif line_has_attr('house ', line) and house is None:\n",
    "                    house = get_link(line)\n",
    "                elif line_has_attr('issue ', line):\n",
    "                    offspring = get_links(line)\n",
    "                    found_issue = True\n",
    "                elif line_has_attr('birth_date ', line):\n",
    "                    year_search = re.search(r\"\\b\\d{4}\\b\", line)\n",
    "                    birth = year_search[0] if year_search is not None else None\n",
    "                elif line_has_attr('death_date ', line):\n",
    "                    year_search = re.search(r\"\\b\\d{4}\\b\", line)\n",
    "                    death = year_search[0] if year_search is not None else None\n",
    "                elif line_has_attr('reign ', line):\n",
    "                    has_reign = True\n",
    "                    # This takes the first year from the reign\n",
    "                    year_search = re.search(r\"\\b\\d{4}\\b\", line)\n",
    "                    reign_start = year_search[0] if year_search is not None else None\n",
    "            elif line.lower().startswith('{{infobox ') and person_type is None:\n",
    "                # We can extract some basic article type from the infobox configuration\n",
    "                person_type = line[10:].strip().split('|')[0]\n",
    "\n",
    "    node = (name, {'monarch':has_reign, 'house':house, 'reign_start':reign_start, 'birth':birth, 'death':death, 'person_type': person_type, 'parents': parents})\n",
    "    if person_type is not None:\n",
    "      # some weird page types that might end up in the graph due to getting missplaced links\n",
    "      unwanted_pages = ['settlement', 'coat of arms', 'country', 'royal family', 'monarchy',\n",
    "                        'noble house', 'french commune', 'civilian attack',\n",
    "                        'company', 'emblem wide', 'ethnic group', 'family', 'hrhstyles',\n",
    "                        'manner of address', 'religious group', 'german location', 'italian comune',\n",
    "                        'polishcoa', 'royal house', 'artwork', 'diocese', 'river']\n",
    "    for page_type in unwanted_pages:\n",
    "        if page_type in person_type.lower(): return None, None, None, None, None\n",
    "\n",
    "    return node, predecessor, spouses, offspring, parents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb920f22",
   "metadata": {
    "hidden": true,
    "id": "cb920f22"
   },
   "source": [
    "Now we can get the list of initial monarchs, from which we will start the search, as well as initialize the list of nodes and edges as well as the search lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RNflZvDV07Ol",
   "metadata": {},
   "outputs": [],
   "source": [
    "monarchs = list(zip(df['monarch_name'],df['country']))\n",
    "monarchs_old = [('Philippe_of_Belgium','Belgium'), ('Margrethe_II_of_Denmark', 'Denmark'), ('Hans-Adam_II,_Prince_of_Liechtenstein', 'Liechtenstein'),\n",
    "            ('Henri,_Grand_Duke_of_Luxembourg','Luxembourg'), ('Albert_II,_Prince_of_Monaco','Monaco'), ('Willem-Alexander_of_the_Netherlands','Netherlands'),\n",
    "            ('Harald_V_of_Norway','Norway'), ('Felipe_VI','Spain'), ('Carl_XVI_Gustaf','Sweden'), ('Charles_III','UK')]\n",
    "connections = []\n",
    "visited_people = []\n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "# Monarchs who reigned before cutoff are not used\n",
    "cutoff_year = 1450 # 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e09c3",
   "metadata": {
    "hidden": true,
    "id": "ff4e09c3"
   },
   "source": [
    "We can now finally explore the connections of our initial monarchs using a BFS algorithm.\n",
    "\n",
    "The exploration of nodes related via succession is done separately to that of their family connections. This was initially done to explore monarchs in a line of succession differently, and give more importance to the Country of ruling of each. But when growing the graph via more families and older figures, this becomes harder as countries change and so do successions. In this version, all nodes are explored in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the starting monarchs and their predecessors\n",
    "while len(monarchs) > 0:\n",
    "    name, country = monarchs.pop()\n",
    "    if name in visited_people: continue\n",
    "    visited_people.append(name)\n",
    "    node, pre, sps, off, pars = get_node(name)\n",
    "    # If there is no page for the name, skip it (we could add just the name as a node)\n",
    "    if node is None: continue\n",
    "    print(pre,' | ', name)\n",
    "\n",
    "    # This will give all monarchs in the line of predecession the country of the last ruler.\n",
    "    node[1]['country'] = country\n",
    "    # We skip people who died after the cutoff year\n",
    "    if node[1]['death'] is not None and int(node[1]['death']) < cutoff_year:\n",
    "        continue\n",
    "    if node not in nodes:\n",
    "        nodes.append(node)\n",
    "\n",
    "    # Add edges to the predecessor and family connections\n",
    "    if pre is not None:\n",
    "        edges.append((name,pre,{'relationship':'throne'}))\n",
    "    for sp in sps: edges.append((name, sp, {'relationship':'marriage'}))\n",
    "    for o in off: edges.append((name, o, {'relationship':'blood'}))\n",
    "    for p in pars: edges.append((name, p, {'relationship':'blood'}))\n",
    "\n",
    "    # Add predecessor and family connections to the respective lists to be explored\n",
    "    monarchs.append((pre,country))\n",
    "    connections += [spouse for spouse in sps if spouse not in visited_people]\n",
    "    connections += [child for child in off if child not in visited_people]\n",
    "    connections += [parent for parent in pars if parent not in visited_people]\n",
    "\n",
    "print(f'Num monarchs: {len(nodes)}, starting connections: {len(connections)}')\n",
    "# Explore the connections of the found monarchs (and their connections iteratively)\n",
    "while len(connections) > 0:\n",
    "    if(len(connections)%10 == 0): print(f'connections length: {len(connections)}, visited length: {len(visited_people)}')\n",
    "    name = connections.pop()\n",
    "    if name in visited_people:\n",
    "        continue\n",
    "    #print(name)\n",
    "    # This should be checked before adding to the connections list, but I'm too lazy to do it now :)\n",
    "    visited_people.append(name)\n",
    "    node, _, sps, off, pars = get_node(name)\n",
    "    # If there is no page for the name, skip it (we could add just the name as a node)\n",
    "    if node is None:\n",
    "        continue\n",
    "\n",
    "    # Skip people who died before the cutoff year\n",
    "    if node[1]['death'] is not None and int(node[1]['death']) < cutoff_year:\n",
    "        continue\n",
    "    nodes.append(node)\n",
    "\n",
    "    # Add family connections to the respective lists to be explored\n",
    "    for sp in sps: edges.append((name, sp, {'relationship':'marriage'}))\n",
    "    for o in off: edges.append((name, o, {'relationship':'blood'}))\n",
    "    for p in pars: edges.append((name, p, {'relationship':'blood'}))\n",
    "    connections += [spouse for spouse in sps if spouse not in visited_people]\n",
    "    connections += [child for child in off if child not in visited_people]\n",
    "    connections += [parent for parent in pars if parent not in visited_people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oX1qXMbTAHzU",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2b749",
   "metadata": {
    "hidden": true,
    "id": "63f2b749"
   },
   "source": [
    "Links that don't actually have an article and nodes before the threshold year create empty nodes, we'll remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IqL9KRCYVptf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove edges to empty nodes (edges that connect to nodes not in the node set)\n",
    "empty_nodes = [n for n in G.nodes if len(G.nodes[n]) == 0]\n",
    "edges = [e for e in edges if e[0] not in empty_nodes and e[1] not in empty_nodes]\n",
    "print(f'removed {len(empty_nodes)} empty nodes')\n",
    "\n",
    "# Re-generate graph (a bit sloppy, but works and is fast enough)\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c114e984",
   "metadata": {
    "hidden": true,
    "id": "c114e984"
   },
   "source": [
    "Code to store the whole graph or the lists of nodes, edges and dictionary of redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HVM3hzvyD81x",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes())\n",
    "#G.nodes[\"Queen Victoria\"].keys()\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/Graph_1450\"    # path davide\n",
    "with open(os.path.join(base_path, 'graph_imbrscore_1450_new.pkl'), 'wb') as graph:\n",
    "    pickle.dump(G, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MbXIw4KRBhKn",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save graph\n",
    "with open(f'{base_path}/redirects_{cutoff_year}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_redirects, f)\n",
    "with open(f'{base_path}/nodes_{cutoff_year}', 'wb') as f:\n",
    "    pickle.dump(nodes, f)\n",
    "with open(f'{base_path}/edges_{cutoff_year}', 'wb') as f:\n",
    "    pickle.dump(edges, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jfi3BAOCyM9J",
   "metadata": {
    "hidden": true,
    "id": "Jfi3BAOCyM9J"
   },
   "source": [
    "## ATTRIBUTE CREATION (skip if complete graph is loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de30fd6",
   "metadata": {
    "hidden": true,
    "id": "9de30fd6"
   },
   "source": [
    "In this section we add more attributes to each of the built network's nodes for future analysis. In particular, we will add the \"ascendance tree\" for each individual, an inbreeding score calculated using the previous, and text information for their Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smyfNpcizEGM",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "smyfNpcizEGM"
   },
   "source": [
    "### Ascendance Trees and Inbreeding Attributes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "goqIF3C_iuhl",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "goqIF3C_iuhl"
   },
   "source": [
    "#### Ascendance Trees\n",
    "\n",
    "Ascendance trees are family trees that only include progenitors of a certain person. Those are parents, grandparents, grandgrandparents etc... We use them to calculate the inbreeding score which is explained in the network analysis section further in this notebook.\n",
    "\n",
    "An ascendance tree is modeled as a dictionary in which the keys are the names of the progenitors (nodes) and the values are the distance between the origin node and that progenitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z-CmFqhbzCA5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive function to construct ascendance trees. It visits each progenitor until we reach the limit.\n",
    "def fill_ascendance_tree(G, node_string, ascendance_tree, distance):\n",
    "  if node_string in G.nodes:\n",
    "    node_values = G.nodes[node_string]\n",
    "    parents = node_values[\"parents\"]\n",
    "\n",
    "    if len(parents) > 0:\n",
    "      ascendance_tree[parents[0]] = distance\n",
    "      ascendance_tree = fill_ascendance_tree(G, parents[0], ascendance_tree, distance + 1)\n",
    "\n",
    "    if len(parents) > 1:\n",
    "      ascendance_tree[parents[1]] = distance\n",
    "      ascendance_tree = fill_ascendance_tree(G, parents[1], ascendance_tree, distance + 1)\n",
    "\n",
    "    return ascendance_tree\n",
    "\n",
    "  else:\n",
    "    return ascendance_tree\n",
    "\n",
    "# This function initiates the recursive loop for each node\n",
    "def construct_ascendance_trees(G):\n",
    "  for key, value in G.nodes().items():\n",
    "    if G.nodes[key] != {}:\n",
    "      G.nodes[key]['ascendance_tree'] = fill_ascendance_tree(G, key, {}, 1)\n",
    "    else:\n",
    "      G.nodes[key]['ascendance_tree'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EPJ1Y1zUQSnr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code to construct the ascendance trees. KEEP IN MIND THAT IF YOU IMPORTED THE GRAPH FROM A PKL FILE THE ASCENDANCE TREES ARE ALREADY COMPUTED\n",
    "construct_ascendance_trees(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R6gn_eySjDgI",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "R6gn_eySjDgI"
   },
   "source": [
    "#### Inbreeding Coefficient or Score\n",
    "\n",
    "The inbreeding coefficient is a coefficient that we use to measure the amount of inbreeding that a person has. More specificaly, the coefficient of inbreeding of an individual is the probability that two alleles at any locus in an individual are identical by descent from the common ancestor(s) of the two parents (https://en.wikipedia.org/wiki/Coefficient_of_inbreeding).\n",
    "\n",
    "The use and analysis of the inbreeding coefficient are explained later in this notebook in the network analysis section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jBNCCi4h2bC7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_inbreeding = {}\n",
    "#outlier_values_imbr ={}\n",
    "\n",
    "# This function gets the common ancestors of two persons (nodes). It uses the ascendance trees computed previously.\n",
    "def calculate_common_ancestors(node1, node2, G):\n",
    "    if node1 and node2 not in G.nodes():\n",
    "        return None\n",
    "    else:\n",
    "        ancestors1 = set(G.nodes[node1]['ascendance_tree'].keys())\n",
    "        ancestors2 = set(G.nodes[node2]['ascendance_tree'].keys())\n",
    "\n",
    "        common_ancestors = ancestors1 & ancestors2\n",
    "\n",
    "        return common_ancestors\n",
    "\n",
    "\n",
    "# Recursive function to compute the inbreeding coefficient. To compute the inbreeding score we need the inbreeding score of the common ancestors. If that score has not been computed yet, we make a recursive call.\n",
    "# The inbreeding coefficient is set to 0 if less than 1 parent is registered for a node (we can't get common ancestors)\n",
    "# Maybe set a constant to assign to people we can't get the inbreeding of instead of 0\n",
    "\n",
    "def calculate_inbreeding(node_string , G , distance=2):\n",
    "    # Base cases: node not in graph or score already computed\n",
    "    if node_string not in G.nodes():\n",
    "        return 0\n",
    "    if G.nodes[node_string ][\"inbreed_score\"] != -1:\n",
    "        return G.nodes[node_string ][\"inbreed_score\"]\n",
    "\n",
    "    # General case, compute\n",
    "    parents = G.nodes[node_string]['parents'] if G.nodes[node_string]['parents'] is not None else []\n",
    "    if len(parents) == 2  and parents[0] in G.nodes() and parents[1] in G.nodes():\n",
    "        node1 = parents[0]\n",
    "        node2 = parents[1]\n",
    "        # Weird graph errors where a person redirects to spouse\n",
    "        if node1 == node2:\n",
    "            G.nodes[node_string]['inbreed_score'] = 0\n",
    "            return 0\n",
    "    else:\n",
    "        G.nodes[node_string]['inbreed_score'] = 0\n",
    "        return 0\n",
    "\n",
    "    common_ancestors = calculate_common_ancestors(node1, node2, G)\n",
    "    # if list of common ancestors is empty, score = 0\n",
    "    if not common_ancestors:\n",
    "        return 0\n",
    "\n",
    "    # List of common ancestors sorted by distance\n",
    "    ca_dists = []\n",
    "    for ca in common_ancestors:\n",
    "        ancestor_distance_node1 = G.nodes[node1]['ascendance_tree'][ca]\n",
    "        ancestor_distance_node2 = G.nodes[node2]['ascendance_tree'][ca]\n",
    "        total_dist = distance + ancestor_distance_node1 + ancestor_distance_node2\n",
    "        ca_dists.append((ca, total_dist))\n",
    "\n",
    "    ca_dists = sorted(ca_dists, key=lambda item: item[1])\n",
    "\n",
    "    # Use n closest common ancestors\n",
    "    total_inbreeding = 0\n",
    "    for common_ancestor, total_distance in ca_dists[:13]:\n",
    "        imbr_common = calculate_inbreeding(common_ancestor , G)\n",
    "        # ancestor imbreeding score if not plausable value set to 0\n",
    "        if imbr_common  < 0  or imbr_common >1:\n",
    "            imbr_common = 0\n",
    "            total_inbreeding += ( 0.5 ** (total_distance-1) ) * (1+ imbr_common)\n",
    "        else:\n",
    "            total_inbreeding += (0.5 ** (total_distance-1) )* (1+ imbr_common)\n",
    "\n",
    "    # set inbreeding to  -2 if total_inbreeding is  >1.3\n",
    "    if total_inbreeding > 1.3:\n",
    "        return -2\n",
    "    else:\n",
    "        return total_inbreeding\n",
    "\n",
    "\n",
    "# We initialize all inbreed scores to -1 to mark them as not computed.\n",
    "def initialize_inbreed_scores(G):\n",
    "    for key, value in G.nodes().items():\n",
    "        if key in G.nodes():\n",
    "            G.nodes[key]['inbreed_score'] = -1\n",
    "\n",
    "def calculate_inbreed_scores(G):\n",
    "    initialize_inbreed_scores(G)\n",
    "\n",
    "    for key , values in G.nodes.items():\n",
    "        # We only compute it if the value is -1, which means that hasn't been computed yet.\n",
    "        if key in G.nodes() and G.nodes[key]['inbreed_score'] == -1:\n",
    "            score = calculate_inbreeding( key,G)\n",
    "            #set inbreeding score in the graph\n",
    "            G.nodes[key]['inbreed_score'] = score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gGN7UF9s2kjt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code to compute the inbreeding coefficients. KEEP IN MIND THAT IF YOU IMPORTED THE GRAPH FROM A PKL FILE THE INBREEDING COEFFICIENTS ARE ALREADY COMPUTED\n",
    "calculate_inbreed_scores(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwcD5eVNg4R-",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_path = \"/content/drive/MyDrive/Graph_1450\"    # path davide\n",
    "with open(os.path.join(base_path, 'graph_imbrscore_estimate_1450_new.pkl'), 'wb') as graph:\n",
    "    pickle.dump(G, graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YJmPZ0TAum0U",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "YJmPZ0TAum0U"
   },
   "source": [
    "### Text attribute creation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eH1cTEj_6h2B",
   "metadata": {
    "id": "eH1cTEj_6h2B"
   },
   "source": [
    "We have use the following short script to extract the text from all the nodes and save it into files locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g8trUlUCvsHu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the text of the monarch\n",
    "preprocess_dir = r\"C:\\Users\\david\\Desktop\\CORSI ORAAA\\SOCIAL-GRAPH\\Final_project\\monarch_text\"  #from your computer\n",
    "\n",
    "########################### SAVE THE PAGES in local#######################\n",
    "mapping_table = str.maketrans({\"?\":\"-\", \"$\":\"-\", \"/\":\"-\"  ,\"\\\\\":\"-\",  \"*\":\"-\",  \"\\\"\":\"-\",  \"<\":\"-\",\"|\":\"-\",\">\":\"-\",  \":\":\"-\" })\n",
    "file_list=[]  # to haver a list of all downloaded files with their names in filename format!\n",
    "\n",
    "for name in G.nodes():\n",
    "    filename = name.translate(mapping_table)\n",
    "    file_list.append(filename) ## save filenames in a list\n",
    "\n",
    "    file_path = os.path.join(preprocess_dir, filename + \".txt\")\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(file_path):\n",
    "\n",
    "        text = get_wikitext(name)\n",
    "\n",
    "         # save the text in a file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(text))\n",
    "            f.close()\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sEBcUaC1wnxE",
   "metadata": {
    "hidden": true,
    "id": "sEBcUaC1wnxE"
   },
   "source": [
    "**Before running, check if the score dictionaries have been correctyl loaded!**\n",
    "this part has to be run in local with this configuration! otherwise, all text information has to be uploaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vkUHZp2B6v-M",
   "metadata": {
    "id": "vkUHZp2B6v-M"
   },
   "source": [
    "Now we can define some auxiliary functions to process the downloaded texts (remove punctuation, set to lowercase...) and do some computation over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NiqUg06Hv1Oe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "def remove_punctuation_lower(tokens):\n",
    "\n",
    "    # Define a string containing all punctuation characters\n",
    "    punctuation_chars = string.punctuation +\"'' ``\"\n",
    "\n",
    "    # Remove all punctuation characters from the list of tokens\n",
    "    tokens_without_punctuation = [token.lower()  for token in tokens if token not in punctuation_chars]\n",
    "    return tokens_without_punctuation\n",
    "\n",
    "# function used to count the number of words in te text\n",
    "def count_words_NLTK(wikitext):\n",
    "    # without nltk  regular expressions are used :\n",
    "    #words_pattern = r'\\b\\w+\\b'\n",
    "    #words = re.findall(words_pattern, wikitext)\n",
    "\n",
    "    # with nltk\n",
    "    words=word_tokenize(wikitext)           # WITH nltk\n",
    "    words= remove_punctuation_lower(words)  # set to lowercase, remove punctuation\n",
    "    return len(words), words      # return the number of words and the list of words/tokens\n",
    "\n",
    "#  Load LabMT scores from a file into a dictionary\n",
    "def load_labmt_scores_FILE(filename , lines_to_skip=4 ):\n",
    "    labmt_scores = {}\n",
    "    with open(filename, 'r', encoding='utf-8' ) as file:\n",
    "        for _ in range(lines_to_skip):  # skip the first 4 lines(title, header, etc.)\n",
    "            next(file)\n",
    "        for line in file:\n",
    "            liness= line.strip().split() # split the line into a list of strings\n",
    "            #extract  0: word, 2: happiness_average\n",
    "            word, score = liness[0], liness[2]\n",
    "            labmt_scores[word] = float(score)  # add the word and its score to the dictionary\n",
    "    return labmt_scores   # return the dictionary\n",
    "\n",
    "# compute sentiment score of a text\n",
    "def compute_sentiment(tokens, dictionary_sentiment_scores):\n",
    "        \"\"\"\n",
    "        Compute the sentiment score of a text based on LabMT word scores.\n",
    "        All tokens has to be already formatted (lower case, no punctuation, etc.)\n",
    "        \"\"\"\n",
    "        #clean the tokens from punctuation and set to lowercase (use the function defined above)\n",
    "        tokens= remove_punctuation_lower(tokens)\n",
    "        score = 0                        # initialize the score to 0\n",
    "        token_len=len(tokens)\n",
    "        for token in tokens:                                  # select each token in the text\n",
    "                if token not in dictionary_sentiment_scores:  # if the token is not in the dictionary, skip it\n",
    "                        token_len-=1             # and don't count it in the average\n",
    "                else:\n",
    "                        score += dictionary_sentiment_scores[token] # add the score of the token to the total score\n",
    "        if token_len==0: return 0\n",
    "        return score/(token_len)  # return the average score for the list of tokens\n",
    "\n",
    "def compute_sentiment_health(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores['compound']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Td36um5nwuca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stored LabMT data from git\n",
    "#file_path_score = \"temp_directory/Data_Set_S1.txt\"\n",
    "# load from pc\n",
    "file_path_score_1=r\"C:\\Users\\david\\Desktop\\CORSI ORAAA\\SOCIAL-GRAPH\\W8\\Data_Set_S1.txt\"\n",
    "file_path_score_2=r\"C:\\Users\\david\\Desktop\\CORSI ORAAA\\SOCIAL-GRAPH\\W8\\Data_Set_S1.txt\"\n",
    "\n",
    "# Load LabMT scores from a file into a dictionary_1\n",
    "sentiment_dict_1= load_labmt_scores_FILE(file_path_score_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I7N6WG897Cv9",
   "metadata": {
    "id": "I7N6WG897Cv9"
   },
   "source": [
    "Finally, we can add all this new text information in the form of token lists to each node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LypNXcQrv-V-",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add text information to nodes in the graph\n",
    "for name in G.nodes():\n",
    "    filename = name.translate(mapping_table)\n",
    "    # extract text from the file\n",
    "    file_path = os.path.join(preprocess_dir, filename + \".txt\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "    count, tokens =count_words_NLTK(text)\n",
    "    s_score_1 = compute_sentiment(tokens, sentiment_dict_1)   # compute sentiment score of the page\n",
    "    s_score_2 = compute_sentiment_health(text)\n",
    "\n",
    "\n",
    "    G.nodes[name]['count'] = count\n",
    "    G.nodes[name]['tokens'] = tokens\n",
    "    G.nodes[name]['sentiment_score_1'] = s_score_1\n",
    "    G.nodes[name]['sentiment_score_2'] = s_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ViybWZ8pwDXD",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check attributes are correctly created\n",
    "G.nodes['Elizabeth II'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l5ogPsZLM4h",
   "metadata": {
    "heading_collapsed": true,
    "id": "8l5ogPsZLM4h"
   },
   "source": [
    "# NETWORK ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370bc31b",
   "metadata": {
    "hidden": true,
    "id": "370bc31b"
   },
   "source": [
    "In this section we perform a basic analysis on the network to understand its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e2154",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "0e3e2154"
   },
   "source": [
    "## Basic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0e859",
   "metadata": {
    "hidden": true,
    "id": "c6d0e859"
   },
   "source": [
    "We will first examine the most basic metrics in our network. The most interesting observation here is that the graph is connected, which means that all people in the graph are connected to each other in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S-diIKtjeT9n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes/edges\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "print('The number of nodes in all the networkare : ',num_nodes)\n",
    "print('The number of edges in all the network are : ',num_edges)\n",
    "print('The number of connected components :', len(list(nx.connected_components(G))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YQXakdcmxYli",
   "metadata": {
    "hidden": true,
    "id": "YQXakdcmxYli"
   },
   "source": [
    "For each node or person, we store a set of attributes for further analysis. We extract these attributes from the Wikipedia infobox. The attributes are the following:\n",
    "- Monarch: Indicates if the person is a King or Queen. This attribute is useful for analysing which royal houses had more positions of power throughout history.\n",
    "\n",
    "- House: Indicates the house of the royal (for example, House of Bourbon). It is useful again for further data analysis and visualization.\n",
    "\n",
    "- Reign Start: The date in which the reign started, in case the person was a Monarch. Can be useful for data visualization and data filtering.\n",
    "\n",
    "- Birth and Death: Dates of birth and death respectively. It can also be used for data visualization and filtering.\n",
    "\n",
    "- Parents: Holds references to the parents nodes of the person. This is useful to construct ascendance trees as we will see further in the notebook.\n",
    "\n",
    "- Country: Indicates the country of the royal. It is useful as sometimes a same royal house can rule in different countries. For example, the House of Bourbon is traditionally the french royal house but it currently rules in Spain.\n",
    "\n",
    "Below we can see an example of the node for Felipe VI, the current king of Spain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R4U2MPciN-Cg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the node attributes of Felipe VI King of Spain\n",
    "felipe_VI = G.nodes['Felipe VI']\n",
    "for attribute_key, attr in felipe_VI.items():\n",
    "    if type(attr) not in [list, dict] or len(attr) < 10:\n",
    "        print(f'{attribute_key}: {str(attr)}')\n",
    "    else:\n",
    "        print(f'{attribute_key}: {str(attr[:10]) if type(attr) == list else str(list(attr)[:10])}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae124b40",
   "metadata": {
    "hidden": true,
    "id": "ae124b40"
   },
   "source": [
    "Below we will do a bit more analysis on the properties of the graph, this time focusing on degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Xw4v304N-Hc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Analysis\n",
    "\n",
    "print('The average degree in the network is: ',(num_edges*2)/num_nodes)\n",
    "degs = [len(node_adj[1]) for node_adj in G.adjacency()]\n",
    "plt.hist(degs, align=\"mid\", rwidth=0.9, bins=range(20))\n",
    "plt.title('Initial degree distribution')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(range(20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i2M-lAmWxjCY",
   "metadata": {
    "hidden": true,
    "id": "i2M-lAmWxjCY"
   },
   "source": [
    "**Figure 1.1** <br>\n",
    "In our network, the degree is determined by family and descendance, so it depends mainly on the amount of sons, daughters, and siblings a person has. Because of this, we don't get many extreme values.\n",
    "\n",
    "We see that the average degree is 3.75, which makes sense as a lot of people in the graph still hasn't had any kids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wgu1XfYCxmwk",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "wgu1XfYCxmwk"
   },
   "source": [
    "## Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fbc3e",
   "metadata": {
    "hidden": true,
    "id": "d87fbc3e"
   },
   "source": [
    "Now we will work a bit on visualizing our notebook. They say that an image is worth more than a thousand words, but an image of a network with 7000+ nodes without proper processing is probably worth less than a couple. So we will start by defining styles for nodes and edges according to their propperties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-oWXNanJxhtm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_all_countries_colors(G):\n",
    "  country_colors = {}\n",
    "  for node, values in G.nodes.items():\n",
    "    if 'country' not in values:\n",
    "      continue\n",
    "\n",
    "    country = values['country']\n",
    "    if country not in country_colors:\n",
    "      color = str(hex(random.randrange(0, 2**24)))[2:]\n",
    "      while len(color) < 6:\n",
    "        color = color + \"0\"\n",
    "      country_colors[country]  = '#' + color\n",
    "\n",
    "  return country_colors\n",
    "\n",
    "def get_all_houses_colors(G):\n",
    "  house_colors = {}\n",
    "  for node, values in G.nodes.items():\n",
    "    if 'house' not in values:\n",
    "      continue\n",
    "\n",
    "    country = values['house']\n",
    "    if country not in house_colors:\n",
    "      color = str(hex(random.randrange(0, 2**24)))[2:]\n",
    "      while len(color) < 6:\n",
    "        color = color + \"0\"\n",
    "      house_colors[country]  = '#' + color\n",
    "\n",
    "  return house_colors\n",
    "\n",
    "def get_all_relationship_colors(G):\n",
    "  relationships = [\"blood\", \"marriage\", \"throne\"]\n",
    "  relationship_colors = {}\n",
    "  for relationship in relationships:\n",
    "    color = str(hex(random.randrange(0, 2**24)))[2:]\n",
    "    while len(color) < 6:\n",
    "      color = color + \"0\"\n",
    "\n",
    "    relationship_colors[relationship] = '#' + color\n",
    "  return relationship_colors\n",
    "\n",
    "country_colors = get_all_countries_colors(G)\n",
    "relationship_colors = get_all_relationship_colors(G)\n",
    "house_colors = get_all_houses_colors(G)\n",
    "\n",
    "#print(country_colors)    # we can remove the print here\n",
    "#print(relationship_colors)\n",
    "#print(house_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4c95d",
   "metadata": {
    "hidden": true,
    "id": "b0b4c95d"
   },
   "source": [
    "It will probably be useful to visualize subgraphs of the big one, so we've made a function for just that. This way we can filter with a list of countries or houses. We also have the option to only show people who reigned over something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uNVNWepMxtef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_graph(G, countries, houses, based_on_houses, based_on_countries, only_monarchs):\n",
    "\n",
    "    if based_on_countries:\n",
    "        nodes = (\n",
    "            node\n",
    "            for node, data\n",
    "            in G.nodes(data=True)\n",
    "            if data.get(\"country\") in countries\n",
    "            )\n",
    "    elif based_on_houses:\n",
    "        nodes = (\n",
    "            node\n",
    "            for node, data\n",
    "            in G.nodes(data=True)\n",
    "            if data.get(\"house\") in houses\n",
    "            )\n",
    "    subgraph = G.subgraph(nodes)\n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lTxGrSG0xv4Z",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_countries = [\"Spain\", \"UK\"]\n",
    "graph_houses = [\"House of Bourbon\", \"House of Bourbon-Parma\", \"Spanish royal family\"]\n",
    "based_on_countries = False\n",
    "based_on_houses = True\n",
    "only_monarchs = False\n",
    "with_labels = True\n",
    "\n",
    "filtered_graph = filter_graph(G, graph_countries, graph_houses, based_on_houses, based_on_countries, only_monarchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TsEK-Bu-xzfi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_network(G, based_on_countries, based_on_houses, label_size,figure_size=(8,8)):\n",
    "\n",
    "  country_colors = get_all_countries_colors(G)\n",
    "  countries = country_colors.keys()\n",
    "  relationship_colors = get_all_relationship_colors(G)\n",
    "  house_colors = get_all_houses_colors(G)\n",
    "  houses = house_colors.keys()\n",
    "\n",
    "  relationships = nx.get_edge_attributes(G, \"relationship\", default = \"blood\")\n",
    "\n",
    "  if based_on_countries:\n",
    "    node_colors = [country_colors[person['country']] if 'country'in person.keys() else '#000000' for person in G.nodes().values() ]\n",
    "    patches = [mpatches.Patch(color=color, label=country) for country, color in country_colors.items()] + [mpatches.Patch(color=color, label=relationship) for relationship, color in relationship_colors.items()]\n",
    "\n",
    "  elif based_on_houses:\n",
    "    node_colors = [house_colors[person['house']] if 'house'in person.keys() else '#000000' for person in G.nodes().values() ]\n",
    "    patches = [mpatches.Patch(color=house_colors[house], label=house) for house in houses] + [mpatches.Patch(color=color, label=relationship) for relationship, color in relationship_colors.items()]\n",
    "\n",
    "  edge_colors = [relationship_colors[relationships[edge]] for edge in G.edges() ]\n",
    "  # Temporary oversiplification for easy viewing. Size can be scaled on degree, importance(kings vs plebs), reign length?, ...\n",
    "  node_sizes = [150 if 'country' in person.keys() else 75 for person in G.nodes().values() ]\n",
    "\n",
    "  plt.figure(figsize=figure_size)\n",
    "  pos = nx.spring_layout(G)\n",
    "  nx.draw(G, node_color=node_colors, edge_color = edge_colors, node_size=node_sizes, width = 3, pos = pos)\n",
    "  nx.draw_networkx_labels(G, pos=pos, font_size=label_size)\n",
    "  plt.legend(handles=patches)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf295d0",
   "metadata": {
    "hidden": true,
    "id": "daf295d0"
   },
   "source": [
    "Below we can see all of our previous visualizing functions in action. In this case, we're filtering the graph to only show people from three Bourbon-related royal families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdHGDOskRX9F",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_network(filtered_graph, False, True, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z0BVNnwdlY2n",
   "metadata": {
    "hidden": true,
    "id": "z0BVNnwdlY2n"
   },
   "source": [
    " **Figure 1.2** Visualization of the subgraph formed by people from the houses of Bourbon, Bourbon-Parma and the Spanish royal family (also a branch from the Bourbon family). Nodes without edges are connected to people from other Bourbon branches not present in the visualization.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SV0Iec0NQwn1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "SV0Iec0NQwn1"
   },
   "source": [
    "## Paths and Distances Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8WNd6BTRlit",
   "metadata": {
    "hidden": true,
    "id": "b8WNd6BTRlit"
   },
   "source": [
    "In our initial presentation of our topic, we mentioned that we find the topic of monarchies very interesting because the different monarchies across europe are related to each other and different kings from different countries are often family. In this section we are going to explore this by performing a pathology study and seeing if we can draw paths between different monarchs.\n",
    "\n",
    "We start by trying to calculate the shortest path between two given nodes to see the family relationship between any two given monarchs. We invite you to play and try different monarchs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KW-m30WEHec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of all monarchs in your graph\n",
    "all_monarchs = list(G.nodes)\n",
    "\n",
    "# Create interactive dropdown widgets for choosing origin and destination monarchs\n",
    "origin_dropdown = widgets.Dropdown(\n",
    "    options=all_monarchs,\n",
    "    value=all_monarchs[0],\n",
    "    description='Origin Monarch:',\n",
    ")\n",
    "\n",
    "destination_dropdown = widgets.Dropdown(\n",
    "    options=all_monarchs,\n",
    "    value=all_monarchs[-1],\n",
    "    description='Destination Monarch:',\n",
    ")\n",
    "\n",
    "# Button to trigger the display of the shortest path\n",
    "display_button = widgets.Button(description=\"Display Shortest Path\")\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to update the shortest path based on the selected monarchs\n",
    "def update_shortest_path(change):\n",
    "    origin_monarch = origin_dropdown.value\n",
    "    destination_monarch = destination_dropdown.value\n",
    "\n",
    "    shortest_path = nx.shortest_path(G, source=origin_monarch, target=destination_monarch)\n",
    "    shortest_path_subgraph = G.subgraph(shortest_path)\n",
    "\n",
    "    # Print or visualize the shortest path as needed\n",
    "    print(shortest_path)\n",
    "\n",
    "    with output:\n",
    "        #plt.figure(figsize=(4, 4))\n",
    "        pos = nx.spring_layout(shortest_path_subgraph)\n",
    "        draw_network(shortest_path_subgraph, False, True, label_size = 10, figure_size= (7,4))\n",
    "        plt.show()\n",
    "\n",
    "# Attach the update_shortest_path function to the dropdowns' observe method\n",
    "origin_dropdown.observe(update_shortest_path, names='value')\n",
    "destination_dropdown.observe(update_shortest_path, names='value')\n",
    "\n",
    "# Function to handle button click\n",
    "def on_button_click(b):\n",
    "    output.clear_output()\n",
    "    update_shortest_path(None)\n",
    "\n",
    "# Attach the function to the button's on_click event\n",
    "display_button.on_click(on_button_click)\n",
    "\n",
    "# Display the dropdown widgets and the button\n",
    "display(origin_dropdown)\n",
    "display(destination_dropdown)\n",
    "display(display_button)\n",
    "display(output)\n",
    "\n",
    "# Initial update to show the shortest path for the default monarchs\n",
    "update_shortest_path(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rkMmOrpYmlB7",
   "metadata": {
    "hidden": true,
    "id": "rkMmOrpYmlB7"
   },
   "source": [
    "**Figure 1.3** <br>\n",
    "\n",
    "Interactive visualizer of the shortest path between two people in the graph. Have fun with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vOSyI6pYRnqW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these parameters!\n",
    "origin_monarch = \"Felipe VI\"\n",
    "destination_monarch = \"Margrethe II\"\n",
    "\n",
    "shortest_path = nx.shortest_path(G, source=origin_monarch, target=destination_monarch)\n",
    "\n",
    "shortest_path_subgraph = G.subgraph(shortest_path)\n",
    "print(shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IzqXsP_kRquk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try and draw the network so you can visualize the different houses and types of relationships between the nodes.\n",
    "\n",
    "draw_network(shortest_path_subgraph, False, True, label_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DZPhHPpeRtga",
   "metadata": {
    "hidden": true,
    "id": "DZPhHPpeRtga"
   },
   "source": [
    "After having played a little bit with the paths between two specific monarchs, let's now compute the average path length for all nodes, which is useful to see how well connected the network is and if we can draw a short path between any pair of given monarchs or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snogJbdCRwR-",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the average shortest path, we can only use connected graphs.\n",
    "# In our case, it is. But by keeping this line we can generalize to other variations.\n",
    "connected_components = nx.connected_components(G)\n",
    "print(\"The size of the most connected components of the graph is: \" + str([len(c) for c in sorted(connected_components, key=len, reverse=True)]))\n",
    "\n",
    "# If we take a look at the components, we will get two of them. The biggest one is way bigger than the second one, this is why we will ignore the second subcomponent for now and focus on the biggest one.\n",
    "GCC = [G.subgraph(c).copy() for c in nx.connected_components(G)][0]\n",
    "avg_shortest_path = nx.average_shortest_path_length(GCC)\n",
    "print(\"The average shortest path for the biggest connected component is: \" + str(avg_shortest_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j34APfzhRymH",
   "metadata": {
    "hidden": true,
    "id": "j34APfzhRymH"
   },
   "source": [
    "This average shortest path makes a lot of sense, because even though most monarchs are connected between each other (predecessor-successor link) , normally you will have to go minimum 3 generations back to find the common family member.\n",
    "\n",
    "\n",
    "Another parameter that can tell us more about the structure of our graph is the cluster coefficient, which captures the degree to which the neighbors of a given node link to each other. We expect this to be low as we are modeling family relationships and even though they are very connected, the neighbors of a certain node don't tend to be connected very much between them because for example a granfather won't be connected (directly) to their grandson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i1P15O51R2E7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient = nx.average_clustering(GCC)\n",
    "print(\"The average clustering coefficient is: \" + str(clustering_coefficient))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vWEI2gdaR4uH",
   "metadata": {
    "hidden": true,
    "id": "vWEI2gdaR4uH"
   },
   "source": [
    "As expected, it is not very high. But despite that, it is still higher than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iudpoFLBx7NX",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "iudpoFLBx7NX"
   },
   "source": [
    "## Within the context of Monarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YrvJXwcwR73r",
   "metadata": {
    "hidden": true,
    "id": "YrvJXwcwR73r"
   },
   "source": [
    "For centuries members of europen royal families often married closed relatives. This practice helped to consolidate power, titles and thrones. But as science has advanced, we now know that this can lead to problems regarding DNA and birth defects.\n",
    "\n",
    "For our project, we wanted to use our network to perform an analysis on inbreeding in the context of royal families across Europe. To do this, we will be computing the coefficient of inbreeding (https://en.wikipedia.org/wiki/Coefficient_of_inbreeding) for each royal, to see if we come up with interesting results.\n",
    "\n",
    "To compute the coefficient of inbreeding, we first need to compute the ascendance trees for each royal, which will help us to determine if two royals have ancestors in common.\n",
    "\n",
    "After this, we will apply the formula which uses the distances between the target royal and the common ancestors of its parents and the inbreeding score of those common ancestors.\n",
    "\n",
    "You can see the computation of ascendance trees and inbreeding scores in the network construction section under the attribute creation subsection.\n",
    "\n",
    "An interesting fact to keep in mind now that we're starting this section is that the average inbreeding score in society is [1% according to this study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gRwahHq4ruF2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a list of all the inbreeding scores and multiply by 100 to have a percentage\n",
    "node_names = list(G.nodes())\n",
    "inbreed_scores = [G.nodes[node]['inbreed_score']*100 for node in node_names]\n",
    "inbreed_scores_NOzeros = [i for i in inbreed_scores if i != 0  ]\n",
    "\n",
    "# set the threshold value for which the imbreading score is considered high\n",
    "#probability of having a child with a genetic disorder > ...% and\n",
    "# lower threshold for which the imbreading score is considered low = not affecting the health of the child in any way\n",
    "\n",
    "\n",
    "#  WE HAVE TO CHOSE THE THRESHOLD VALUE!!!! not the one below that are just for testing\n",
    "threshold_up = 15 #np.mean(imbreed_scores_NOzeros) + 2*np.std(imbreed_scores_NOzeros)\n",
    "threshold_down = 1\n",
    "\n",
    "\n",
    "\n",
    "# function used to calculate the occurency of words in a list and set in a dictionary\n",
    "def occurencies(list):\n",
    "    occurencies = {}\n",
    "    for i in list:\n",
    "        occurencies[i] = occurencies.get(i, 0) + 1\n",
    "    return occurencies\n",
    "\n",
    "# plot subplot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(11, 4)\n",
    "# text scale\n",
    "fig.suptitle('Inbreeding Score Distribution', fontweight='bold', fontsize=16)\n",
    "\n",
    "# ax1\n",
    "ax1.hist(inbreed_scores_NOzeros, bins=50)\n",
    "ax1.set_title(\"Distribution without 0 score values\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_xlabel(\"Inbreeding score\")\n",
    "ax1.axvline(threshold_up, color='red', linestyle='-', linewidth=1, label='Threshold Up (>15)')\n",
    "ax1.axvline(threshold_down, color='green', linestyle='-', linewidth=1, label='Threshold Down (<1)')\n",
    "ax1.legend()\n",
    "# x ticks\n",
    "#\n",
    "\n",
    "# ax2 values and frequency\n",
    "k_conf = occurencies(inbreed_scores).keys()\n",
    "freq_conf = occurencies(inbreed_scores).values()\n",
    "\n",
    "# ax2\n",
    "ax2.plot(k_conf, freq_conf, \".\")\n",
    "ax2.set_title(\"Complete Distribution\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "ax2.set_xlabel(\"Inbreeding score\")\n",
    "ax2.axvline(threshold_up, color='red', linestyle='-', linewidth=1, label='Threshold Up (>15)')\n",
    "ax2.axvline(threshold_down, color='green', linestyle='-', linewidth=1, label='Threshold Down(<1)')\n",
    "ax2.legend()\n",
    "# ax2.set_xticks(range(0, int(max(imbreed_scores))))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"Nodes with inbreeding score > {threshold_up} :\", len([i for i in inbreed_scores if i > threshold_up ]))\n",
    "print(f\"Nodes with inbreeding score < {threshold_down} :\", len([i for i in inbreed_scores if i < threshold_down]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "th-cxTmYo95S",
   "metadata": {
    "hidden": true,
    "id": "th-cxTmYo95S"
   },
   "source": [
    "**Figure 1.4** <br>\n",
    "The figure above shows the Inbreeding Score Distribution both :\n",
    "* cleaned from 0 imbreeding score values (for better visualization of the distibution shape);\n",
    "*  the complete distibution.\n",
    "How it could be seen, the two separate visualizations are needed as the frequency of the 0 value of the inbreeding score its in a completly different order of magnitude.\n",
    "\n",
    "While the vast majority of nodes have Inbreeding scores around 0, 112 nodes have been selected as having extremely high Inbreeding Scores  (value>20), which means they have had from  1/5 to ~ 1/2 chance of inheriting the same allel from both parents. Which, if he end up to be having recesseive treat in the homoxigotic allel , it could have lead to extremely high health issues (e.g : Cystic Fibrosis, Sickle Cell Anemia,\n",
    "Thalassemia and much others)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fyV8zArWOknj",
   "metadata": {
    "hidden": true,
    "id": "fyV8zArWOknj"
   },
   "source": [
    "Following, are printed out the 10 people with higher imbreeding score as well as the number of nodes with inbreeding score over 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SdEiiAxmiE5Q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the nodes names and put in a list\n",
    "node_names = list(G.nodes())\n",
    "\n",
    "\n",
    "#create diuctionary with all nodes names and scores\n",
    "my_dict_imbr = dict(zip(node_names, inbreed_scores))\n",
    "sorted_dict_imbr = dict(sorted(my_dict_imbr.items(), key=lambda item: item[1], reverse=True)) # Sort the dictionary by values in descending order\n",
    "\n",
    "# Visualize the 10 persons with higher inbreeding score :\n",
    "first_10_items = dict(list(sorted_dict_imbr.items())[:10])\n",
    "print(\"The 10 persons with higher inbreeding score are : \",first_10_items)\n",
    "print(\"\\n\\nThe number of people with an inbreeding score greater than 0.01 is:\", len([i for i in inbreed_scores if i > 0.01]) , \"of a total of\" ,len(node_names),\"nodes.\")\n",
    "print(f\"Which means that {round((len([i for i in inbreed_scores if i > 0.01])/ len(node_names))*100,2)}% of nodes have an inbreeding score greater than 0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nxw6XOD9N_5X",
   "metadata": {
    "hidden": true,
    "id": "nxw6XOD9N_5X"
   },
   "source": [
    "As we can see, the person with the highest inbreeding score is *Charles II of Spain* on of the most well known monarchs due to his severe health problems attributed to inbreeding, which is not surprising given the track record his family - the Habsburgs - had with this practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xHsH0lDQD5KZ",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "xHsH0lDQD5KZ"
   },
   "source": [
    "## Inbreeding throughout time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NlHG57-Tzepp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of nodes devide in two groups: before and after 1750\n",
    "nodes_before =[]\n",
    "nodes_after =[]\n",
    "no_death = []\n",
    "\n",
    "# set threshold years (try to have similar amount of compared data at the end )\n",
    "before= 1701\n",
    "after = 1920\n",
    "\n",
    "for name in G.nodes():\n",
    "    #check if the node has a death date\n",
    "    if G.nodes[name][\"death\"] == None:\n",
    "        # if not the death rate check the birth date, if not discard the node\n",
    "        if G.nodes[name][\"birth\"] == None:\n",
    "            no_death.append(name)\n",
    "            continue\n",
    "        else:\n",
    "            # 60 years before and after is added to the threshold\n",
    "            # to take into account the fact that the are using the birth date i  instead of the death date\n",
    "            if int(G.nodes[name][\"birth\"]) < before-55 and int(G.nodes[name][\"birth\"]) :\n",
    "                nodes_before.append(name)\n",
    "            if int(G.nodes[name][\"birth\"]) > after+55:\n",
    "                nodes_after.append(name)\n",
    "    else:\n",
    "        if int(G.nodes[name][\"death\"]) < before and int(G.nodes[name][\"death\"]) :\n",
    "            nodes_before.append(name)\n",
    "        if int(G.nodes[name][\"death\"]) > after:\n",
    "            nodes_after.append(name)\n",
    "print(\"Here the number of nodes for each time sub-group is wisualized:\")\n",
    "print(f\"Nodes before {before} are  :\", len(nodes_before))\n",
    "print(f\"Nodes after {after} are :\", len(nodes_after))\n",
    "#print(\"Nodes with no death:\", len(no_death))\n",
    "\n",
    "# with ONLY imbreeding score != 0 for visualization purpose\n",
    "imbreed_scores_before = [G.nodes[node]['inbreed_score']*100 for node in nodes_before if G.nodes[node]['inbreed_score'] > 0]\n",
    "imbreed_scores_after = [G.nodes[node]['inbreed_score']*100 for node in nodes_after if (G.nodes[node]['inbreed_score'] > 0)]\n",
    "\n",
    "print(\"Nodes with imbreeding score > 0 before:\", len(imbreed_scores_before))\n",
    "print(\"Nodes with imbreeding score > 0 after:\", len(imbreed_scores_after))\n",
    "\n",
    "#percentage of nodes with imbreeding score < 0\n",
    "print(\"\\n Percentage of nodes with imbreeding score < 0 before:\", round((len(nodes_before)- len(imbreed_scores_before))/len(nodes_before)*100 ,0), \"%\")\n",
    "print(\"\\n Percentage of nodes with imbreeding score < 0 after:\",round( (len(nodes_after)- len(imbreed_scores_after))/len(nodes_after)*100 ,0), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vV7ZigVED5wC",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subplot comparing imbreed_scores_after_1750 against imbreed_scores_before_1750 distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4), sharey=True)\n",
    "\n",
    "# Figure title\n",
    "fig.suptitle(f'Inbreeding Score Distribution after {after} and before {before}', fontweight='bold', fontsize=16)\n",
    "\n",
    "# Create histograms for each set of scores\n",
    "hist_after_1750 = ax1.hist(imbreed_scores_after, bins=100, color='blue', alpha=0.7, label='After 1750')\n",
    "hist_before_1750 = ax2.hist(imbreed_scores_before, bins=100, color='red', alpha=0.7, label='Before 1750')\n",
    "\n",
    "# Set x-axis limits for better comparison\n",
    "x_min, x_max = min(min(imbreed_scores_before),min(imbreed_scores_after)), max(max(imbreed_scores_before), max(imbreed_scores_after))+1\n",
    "ax1.set_xlim(x_min, x_max)\n",
    "ax2.set_xlim(x_min, x_max)\n",
    "\n",
    "# Set x-axis ticks at intervals of 10\n",
    "xticks_interval = 5\n",
    "ax1.set_xticks(np.arange(x_min, x_max + 1, xticks_interval))\n",
    "ax2.set_xticks(np.arange(x_min, x_max + 1, xticks_interval))\n",
    "\n",
    "# Calculate statistics for each set of scores\n",
    "mean_after = np.mean(imbreed_scores_after)\n",
    "mean_before= np.mean(imbreed_scores_before)\n",
    "median_after = np.median(imbreed_scores_after)\n",
    "median_before = np.median(imbreed_scores_before)\n",
    "percentile_10_after= np.percentile(imbreed_scores_after, 10)\n",
    "percentile_10_before = np.percentile(imbreed_scores_before, 10)\n",
    "percentile_90_after = np.percentile(imbreed_scores_after, 90)\n",
    "percentile_90_before = np.percentile(imbreed_scores_before, 90)\n",
    "\n",
    "# Add vertical lines for the statistics\n",
    "ax1.axvline(mean_after, color='black', linestyle='solid', linewidth=1)\n",
    "ax1.axvline(median_after, color='violet', linestyle='solid', linewidth=1)\n",
    "ax1.axvline(percentile_10_after, color='green', linestyle='dashed', linewidth=1)\n",
    "ax1.axvline(percentile_90_after, color='green', linestyle='dashed', linewidth=1)\n",
    "\n",
    "ax2.axvline(mean_before , color='black', linestyle='solid', linewidth=1, label='Dashed lines')\n",
    "ax2.axvline(median_before, color='violet', linestyle='solid', linewidth=1)\n",
    "ax2.axvline(percentile_10_before, color='green', linestyle='dashed', linewidth=1)\n",
    "ax2.axvline(percentile_90_before, color='green', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Add legends, labels, and titles to the plot\n",
    "ax1.set_xlabel('Imbreeding score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title(f'After  {after} ')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_xlabel('Imbreeding score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title(f'Before  {before} ')\n",
    "ax2.legend()\n",
    "# list of nodes devide in two groups: before and after 1750\n",
    "nodes_before =[]\n",
    "nodes_after =[]\n",
    "no_death = []\n",
    "\n",
    "# set threshold years (try to have similar amount of compared data at the end )\n",
    "before= 1701\n",
    "after = 1920\n",
    "\n",
    "for name in G.nodes():\n",
    "    #check if the node has a death date\n",
    "    if G.nodes[name][\"death\"] == None:\n",
    "        # if not the death rate check the birth date, if not discard the node\n",
    "        if G.nodes[name][\"birth\"] == None:\n",
    "            no_death.append(name)\n",
    "            continue\n",
    "        else:\n",
    "            # 60 years before and after is added to the threshold\n",
    "            # to take into account the fact that the are using the birth date i  instead of the death date\n",
    "            if int(G.nodes[name][\"birth\"]) < before-55 and int(G.nodes[name][\"birth\"]) :\n",
    "                nodes_before.append(name)\n",
    "            if int(G.nodes[name][\"birth\"]) > after+55:\n",
    "                nodes_after.append(name)\n",
    "    else:\n",
    "        if int(G.nodes[name][\"death\"]) < before and int(G.nodes[name][\"death\"]) :\n",
    "            nodes_before.append(name)\n",
    "        if int(G.nodes[name][\"death\"]) > after:\n",
    "            nodes_after.append(name)\n",
    "print(\"Here the number of nodes for each time sub-group is wisualized:\")\n",
    "print(f\"Nodes before {before} are  :\", len(nodes_before))\n",
    "print(f\"Nodes after {after} are :\", len(nodes_after))\n",
    "#print(\"Nodes with no death:\", len(no_death))\n",
    "\n",
    "# with ONLY imbreeding score != 0 for visualization purpose\n",
    "imbreed_scores_before = [G.nodes[node]['inbreed_score']*100 for node in nodes_before if G.nodes[node]['inbreed_score'] > 0]\n",
    "imbreed_scores_after = [G.nodes[node]['inbreed_score']*100 for node in nodes_after if (G.nodes[node]['inbreed_score'] > 0)]\n",
    "\n",
    "print(\"Nodes with imbreeding score > 0 before:\", len(imbreed_scores_before))\n",
    "print(\"Nodes with imbreeding score > 0 after:\", len(imbreed_scores_after))\n",
    "\n",
    "#percentage of nodes with imbreeding score < 0\n",
    "print(\"\\n Percentage of nodes with imbreeding score < 0 before:\", round((len(nodes_before)- len(imbreed_scores_before))/len(nodes_before)*100 ,0), \"%\")\n",
    "print(\"\\n Percentage of nodes with imbreeding score < 0 after:\",round( (len(nodes_after)- len(imbreed_scores_after))/len(nodes_after)*100 ,0), \"%\")\n",
    "\n",
    "# Add legends, labels, and title to the plot\n",
    "colors = {\"Mean\": 'black', \"Median\": 'violet', \"10th percentile\": 'green', \"90th percentile\": 'green'}\n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Line2D([0], [0], color=colors[label], linestyle='-' if label == \"Mean\" or label ==\"Median\" else '--') for label in labels]\n",
    "ax1.legend(handles, labels, fontsize=6,)\n",
    "ax2.legend(handles, labels, fontsize=6,)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Median before:\", round(median_before,2))\n",
    "print(\"Median after:\", round(median_after,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xiHRpmOMzljL",
   "metadata": {
    "id": "xiHRpmOMzljL"
   },
   "source": [
    " **Figure 1.5**\n",
    "\n",
    "In the figure above a inbreeding score distibution comparison between the two sub-gourps (lived before 1701 /ref{} and after 1920 /ref{}) is done.\n",
    "Two threshold were chosen to extract two sub-groups of similiar size (after removing 0 inbreeding score nodes) to create a visual comparable distibution.      In addition, the two dates where chosen as being two historical thresholds well known for being breaking points of scientific and social changes (start of 18th century, the \"Roaring Twenties\")\n",
    "\n",
    "Analysing the figure it could be noticed that differently as it was expectedat at the beginning the median value for the gorup after 1900 result even higher that the one of before 1700.(Our first guess was to find a smaller value as inbreeding European Royal families back in the day is a really well known fact.) However, taking a closer look and making more research about inbreeding score the found  distibution could be really well explaned with the inbreeding score theroy. In fact, the inbreeding score for a population is a factor that grows in time if inbreeding behaviour continue,even with very distance relatives (as seen in the inbreeding formula) , which well explain the general slight shift of the \"after 1920\"  distibution to slightly higher values coming to the lower end of the distibution (It could be also noticed, that the $%$ of nodes with inbreeding score = 0 drop from  $78%$ to arounf $55%$). In the same time, due to social changes and scientifical  discoveries about the genetic risk of inbreeding, the extreem high values (>15) are significatelly dropped. It could also be noticed that the only two oulier values with inbreeding score (>15) in the \"After 1920\" distibution belong to   Princess Maria Antonietta of Bourbon-Two Sicilies and Princess Maria Carolina of Bourbon-Two Sicilies are both part of the same House of Bourbon-Two Sicilies; well known for their high inbreeding history and discovered to be one of the most inbred families \\ref{fig:inbreed_score_distribution}.\n",
    "\n",
    "    However, it has to be taken in consideration that our inbreeding score for the more recent person will consider (for a small amount) the inbreeding score of the past people. So people with time get more and more imbreeded in some sort (is this a helth issue in long run? Check [imbreeding scientific article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5916862/))\n",
    "    **still strange!!! we have to find a possible explantion!!**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"The 1900s Science and Technology: Overview .\" U*X*L American Decades. . Retrieved November 15, 2023 from Encyclopedia.com: https://www.encyclopedia.com/social-sciences/culture-magazines/1900s-science-and-technology-overview\n",
    "\n",
    "\n",
    "Osler, Margaret J. , Spencer, J. Brookes and Brush, Stephen G.. \"Scientific Revolution\". Encyclopedia Britannica, 26 Nov. 2019, https://www.britannica.com/science/Scientific-Revolution. Accessed 6 December 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PddU9pKYNjQL",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "PddU9pKYNjQL"
   },
   "source": [
    "## Community and Family analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d686de1",
   "metadata": {
    "hidden": true,
    "id": "5d686de1"
   },
   "source": [
    "We will now analyze the connectedness of royal families and their relationship with inbreeding. It's worth noting that due to how Wikipedia sets a person's house and how it treats sub-families/branches, there are more general and more fine-grained families, which could be joined into one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qiY_2uEKZiEJ",
   "metadata": {
    "hidden": true,
    "id": "qiY_2uEKZiEJ"
   },
   "source": [
    "With a modularity score we can test how much more connected a subgraph is compared to the original one. This can be an indicator of the existance of communities within a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a66a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity(node_list):\n",
    "    L = G.number_of_edges()\n",
    "    community_graph = G.subgraph(node_list)\n",
    "    Lc = community_graph.number_of_edges()\n",
    "    kc = sum([G.degree(pers) for pers in node_list if pers in G])\n",
    "\n",
    "    return (Lc / L) - (kc / (2*L))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qalksgS2Z6n7",
   "metadata": {
    "hidden": true,
    "id": "qalksgS2Z6n7"
   },
   "source": [
    "We will analyze the average inbreeding score of each royal family, as well as their modularity, to see how well connected they are compared to the rest of the graph. We can see that there was clearly a tendency of some families to abuse inbreeding than others. Some examples of these are the houses of Habsburg and the Spanish Royal family, with average scores over 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses = defaultdict(list)\n",
    "for name, attr in G.nodes().items():\n",
    "    houses[attr['house']].append(name)\n",
    "\n",
    "names = []\n",
    "sizes = []\n",
    "inbreedings = []\n",
    "mods = []\n",
    "for house, people in sorted(houses.items(), key=lambda item: len(item[1]), reverse=True):\n",
    "    inbreeding_scores = [G.nodes[person]['inbreed_score'] for person in people]\n",
    "    names.append(house)\n",
    "    sizes.append(len(people))\n",
    "    mods.append(np.round(modularity(people),5))\n",
    "    inbreedings.append(np.mean(inbreeding_scores))\n",
    "    #print(f'id: {house}, size: {len(people)}, modularity: {modularity(people):.4}, avg inbreeding: {np.mean(inbreeding_scores):.05}')\n",
    "\n",
    "house_df = pd.DataFrame({'name':names, 'size':sizes, 'avg_inbreeding':inbreedings, 'modularity':mods})\n",
    "filtered_house_df = house_df[house_df['size'] > 1]\n",
    "sorted_filtered_df = filtered_house_df.sort_values(by=['avg_inbreeding'],ascending=False)\n",
    "display(sorted_filtered_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796cfc9",
   "metadata": {
    "hidden": true,
    "id": "6796cfc9"
   },
   "source": [
    "**Table 2.1** <br>\n",
    "\n",
    "Table of royal families and some of their metrics, sorted by their average inbreeding score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947b294",
   "metadata": {
    "hidden": true,
    "id": "8947b294"
   },
   "source": [
    "We can also use a community detection algorithm such as louvain to see how different royal houses are joined into bigger ones. We can see, that as expected, some royal families form bigger ones. One example of this is the various branches of the Bourbon family uniting into one of the communities of highest modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "from collections import defaultdict\n",
    "\n",
    "coms = community_louvain.best_partition(G)\n",
    "communities = defaultdict(list)\n",
    "community_houses = defaultdict(lambda: defaultdict(int))\n",
    "for pers, com in coms.items():\n",
    "    communities[com].append(pers)\n",
    "    community_houses[com][G.nodes[pers]['house']]+=1\n",
    "\n",
    "ids = []\n",
    "sizes = []\n",
    "mods = []\n",
    "top_houses = []\n",
    "total_houses = []\n",
    "\n",
    "num_houses = 4\n",
    "for community, people in sorted(communities.items(), key=lambda item: modularity(item[1]), reverse=True):\n",
    "    houses = sorted(community_houses[community].items(), key=lambda item: item[1], reverse=True)\n",
    "    ids.append(community)\n",
    "    sizes.append(len(people))\n",
    "    mods.append(np.round(modularity(people),5))\n",
    "    top_houses.append(houses[:num_houses])\n",
    "    total_houses.append(len(houses))\n",
    "    #print(f'id: {community}, size: {len(people)}, modularity: {modularity(people):.4}, houses:{houses[:num_houses]} + {len(houses)-num_houses} more')\n",
    "\n",
    "community_df = pd.DataFrame({'id':ids, 'size':sizes, 'modularity':mods, 'top_houses':top_houses, 'num_houses':total_houses})\n",
    "display(community_df[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SkYEHAD5CzVd",
   "metadata": {
    "hidden": true,
    "id": "SkYEHAD5CzVd"
   },
   "source": [
    "**Table 2.2** <br>\n",
    "\n",
    "Table of communities found within our network using the Louvain algorithm and the houses that are cointained within them. Sorted by modularity, so how well connected they are to the rest of the graph, \"how well they form a community\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5kQpfpEXEpKF",
   "metadata": {
    "heading_collapsed": true,
    "id": "5kQpfpEXEpKF"
   },
   "source": [
    "# TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "te13AfW-7aQb",
   "metadata": {
    "id": "te13AfW-7aQb"
   },
   "source": [
    "1.0) Corpus creation\n",
    "\n",
    "1)Wikitext exploration.\n",
    "\n",
    "In this section a firstly a deep analysis of the all the wiki text will be performed to to extract a broad information about the overall word corpus:\n",
    "\n",
    "\n",
    "\n",
    "* corpus and dictionary word  lenght\n",
    "* lexical diversity among the corpus\n",
    "* most common used words\n",
    "\n",
    "2)Following, it could be found a wordcloud-plot comparison between high and low inbreeding scores groups.\n",
    "\n",
    "3)Finally, an evaluation of the previously calculated sentiment score against high and low imbreeding score is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RURqf5sGSr29",
   "metadata": {
    "hidden": true,
    "id": "RURqf5sGSr29"
   },
   "source": [
    "## Wiki text exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sa_GkTrw7gVC",
   "metadata": {
    "id": "sa_GkTrw7gVC"
   },
   "source": [
    " This sub-section collects the wikitext of all nodes into a corpus. After tokenization, all words are converted to lowercase and all punctuation is removed. Finally, a customized list of stopwords (nltk english stopwords + frequent wikipedia formatting tokens) is removed for a clean dataset. This process prepares the corpus for efficient natural language processing and analysis within the network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qSND-1i6GJx7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder  where are saved the txt files with the text of the monarchs\n",
    "preprocess_dir = r\"C:\\Users\\david\\Desktop\\CORSI ORAAA\\SOCIAL-GRAPH\\Final_project\\monarch_text\"  # for pc\n",
    "#preprocess_dir = r\"/content/drive/MyDrive/Graph_1450/monarch_text\"\n",
    "#preprocess_dir = '/content/drive/MyDrive/DTU/Social Graphs 2023/monarch_text'\n",
    "\n",
    "corpus_root = preprocess_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I56nrlsMF-i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word list creation:\n",
    "# Extraction of downloaded nltk english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# addition of customized stopwords\n",
    "list_remove =[\"ref\",\"url\", \"p\", \"http\",\"https\",\"www\", \"org\", \"com\", \"publisher\", \"file\", \"page\", \"html\", \"jpg\", \"br\", \"sfn\"]\n",
    "stop_words.update(list_remove)\n",
    "\n",
    "# Function to calculate vocabulary size\n",
    "def vocab_size(text):\n",
    "    \"\"\" text: nltk.Text object\"\"\"\n",
    "\n",
    "    words = text.tokens\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words)\n",
    "\n",
    "# function to remove punctuation\n",
    "def remove_punctuation(tokens):\n",
    "    # Define a string containing all punctuation characters\n",
    "    punctuation_chars = string.punctuation +\"'' ``\"\n",
    "\n",
    "    # Remove all punctuation characters from the list of tokens\n",
    "    tokens_without_punctuation = [token  for token in tokens if token not in punctuation_chars]\n",
    "    return tokens_without_punctuation\n",
    "\n",
    "# set lower case\n",
    "def set_lower_case(tokens):\n",
    "    tokens_lowercase = [token.lower() for token in tokens]\n",
    "    return tokens_lowercase\n",
    "\n",
    "##remove punctuation and set lower case functions ##############################\n",
    "\n",
    "def remove_punctuation_lower(tokens):\n",
    "    # Define a string containing all punctuation characters\n",
    "    punctuation_chars = string.punctuation +\"'' ``\"\n",
    "\n",
    "    # Remove all punctuation characters from the list of tokens\n",
    "    tokens_without_punctuation = [token.lower()  for token in tokens if token not in punctuation_chars]\n",
    "    return tokens_without_punctuation\n",
    "\n",
    "# function used to calculate the occurency of words in a list and set in a dictionary\n",
    "def occurencies(list):\n",
    "    occurencies = {}\n",
    "    for i in list:\n",
    "        occurencies[i] = occurencies.get(i, 0) + 1\n",
    "    return occurencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faNt74AF-ss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus creation\n",
    "corpus = PlaintextCorpusReader(corpus_root, \".*\")\n",
    "all_files_list = corpus.fileids() # list of alle the files in the corpus (\"monarch corpus\")\n",
    "corpus_raw = corpus.raw() # raw text of the entire corpus (\"monarch corpus\")\n",
    "\n",
    "# TOKENIZATION of the corpus\n",
    "\n",
    "# takes to long to run (more then 30 minutes and still running!!)\n",
    "#tokens = nltk.word_tokenize(corpus_raw) # tokenized corpus = list of tokens\n",
    "wordlist = corpus.words()  # list of all the words/tokens in the corpus (\n",
    "\n",
    "tokens_clean_lower = remove_punctuation_lower(wordlist)  # remove punctuation and set lower case\n",
    "# create a text object from the corpus\n",
    "text_obj = nltk.Text(tokens_clean_lower)\n",
    "\n",
    "# vocabulary size (number of unique tokens in the corpus)\n",
    "tokens_vocabulary_len = vocab_size(text_obj)\n",
    "\n",
    "\n",
    "##### stop words- removal\n",
    "tokens_clean_lower_NoStopWords = [word for word in tokens_clean_lower if word.isalpha() and word not in stop_words]\n",
    "text_obj_NoStopWords = nltk.Text(tokens_clean_lower_NoStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JZCrtBlfWese",
   "metadata": {
    "hidden": true,
    "id": "JZCrtBlfWese"
   },
   "source": [
    "1) Wikitext exploration:\n",
    "\n",
    "To get a general overview of the analyzed corpus:\n",
    "\n",
    "the corpus lenght (number of tokens in the corpus),  the number of unique tokens and the lexical diversity is extract*.\n",
    "\n",
    "*The lexical diveristy is defined as :\n",
    "\n",
    "$lexical\\_diversity = distinct\\_tokens / N\\_tokens $ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eVVjBJhZWd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The lenght of the tokens in the corpus  before cleaning is :\", len(wordlist) )\n",
    "print(\"\\n The Vocabulary of all the cleaned corpus has size:\", tokens_vocabulary_len )\n",
    "\n",
    "# lexical diversity among all the wikipedia pages\n",
    "def lexical_diversity(text):\n",
    " return len(set(text)) / len(text)  #different vocabulary used/all words used\n",
    "\n",
    "# calculate the percentage\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total #percentage\n",
    "\n",
    "div= lexical_diversity(text_obj)\n",
    "print(\"\\n Lexical diversity:\", div,  f\"\\n It means that only {round(div*100,3)} % of the words are unique, the other {100- round(div*100,3)} % of the text is composed of the repetition of these words. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gFORECZFHAWS",
   "metadata": {
    "hidden": true,
    "id": "gFORECZFHAWS"
   },
   "source": [
    "## Most common words plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tn8HON_3F-6y",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_words(tokens_clean_lower,n=75):\n",
    "    # Tokenize the text (get all tokens from the text)\n",
    "    words = tokens_clean_lower\n",
    "\n",
    "    # Get the English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Remove stopwords and non-alphabetic words  (isalpha() returns True if all characters in the string are alphabetic\n",
    "    # then check if the word is not in the stop_words list) if both condition are True acccept the word\n",
    "    #filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    filtered_words = [word.lower() for word in words if word.isalpha() ]\n",
    "\n",
    "    # Create a frequency distribution of the filtered words\n",
    "    fdist = nltk.FreqDist(filtered_words)\n",
    "\n",
    "    # Get the 75 most frequent words (from FreqDist object!)\n",
    "    most_common_words = fdist.most_common(n)\n",
    "\n",
    "    return most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JYYQd2QHF_B-",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_non_stopwords__1(tokens_clean_lower,n=75):\n",
    "    # Tokenize the text (get all tokens from the text)\n",
    "    words = tokens_clean_lower\n",
    "\n",
    "    # Get the English stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    list_remove =[\"ref\",\"url\", \"p\", \"http\", \"https\",\"www\", \"org\", \"com\", \"publisher\", \"file\", \"page\", \"html\", \"jpg\", \"br\", \"sfn\", \"ii\", \"iii\",\"pp\"]\n",
    "    stop_words+= list_remove\n",
    "\n",
    "    # Remove stopwords and non-alphabetic words  (isalpha() returns True if all characters in the string are alphabetic\n",
    "    # then check if the word is not in the stop_words list) if both condition are True acccept the word\n",
    "    filtered_words_1 = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    #filtered_words_1 = [word.lower() for word in filtered_words_1 if word.lower() not in stop_words]\n",
    "\n",
    "    # Create a frequency distribution of the filtered words\n",
    "    fdist = nltk.FreqDist(filtered_words_1)\n",
    "\n",
    "    # Get the 75 most frequent words (from FreqDist object!)\n",
    "    most_common_words_1 = fdist.most_common(n)\n",
    "\n",
    "    return most_common_words_1\n",
    "\n",
    "most_frequent_words__1 = frequency_non_stopwords__1(tokens_clean_lower)\n",
    "\n",
    "# plot the histogram of the 75 most common words\n",
    "k_1 ,freq_1 = zip(*most_frequent_words__1[:]) # remove the word  \"ref\"\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2,1)\n",
    "fig.suptitle('Occurencies of the 75 most frequent Not Stopwords in the entire Monarchs\\' Corpus')\n",
    "# text scale\n",
    "fig.set_size_inches(14, 7)\n",
    "\n",
    "\n",
    "ax1.plot(k_1, freq_1, \".\")\n",
    "\n",
    "ax1.set_ylabel('Occurencies')\n",
    "ax1.set_xlabel('Word Tokens')\n",
    "ax1.set_xticks(range(len(k_1)))\n",
    "ax1.set_xticklabels(k_1, rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "# ax2 cumulative distibution\n",
    "ax2.plot(k_1, np.cumsum(list(freq_1)), \".\")\n",
    "\n",
    "ax2.set_ylabel('Cumulative sum')\n",
    "ax2.set_xlabel('Word Tokens')\n",
    "ax2.set_xticks(range(len(k_1)))\n",
    "ax2.set_xticklabels(k_1, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YMLt3bB6C4xz",
   "metadata": {
    "hidden": true,
    "id": "YMLt3bB6C4xz"
   },
   "source": [
    "**Figure 3.1** <br>  \n",
    "\n",
    "The figure above shows the distibution of the firts 75 more common tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wvWde78YFDK0",
   "metadata": {
    "hidden": true,
    "id": "wvWde78YFDK0"
   },
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7m_q2yFEHaX9",
   "metadata": {
    "hidden": true,
    "id": "7m_q2yFEHaX9"
   },
   "source": [
    "### wordcloud all corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7oYu7OW0FGCY",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_dict_1 = nltk.FreqDist(tokens_clean_lower_NoStopWords)\n",
    "\n",
    "\n",
    "def TF(freq_dict_1, c=0 , normalization=True):\n",
    "    # regolarization factor for balancing the different world count\n",
    "    #tokens count east, tokens count west\n",
    "    count_1 = 1#max(freq_dict_1.values())\n",
    "\n",
    "    TF= []\n",
    "\n",
    "    if normalization:\n",
    "        for i in freq_dict_1.keys():\n",
    "                TF.append( (i,  round(freq_dict_1[i]/count_1,3)  ) )\n",
    "\n",
    "    TF_sorted = sorted(TF, key=lambda tup: tup[1], reverse=True)\n",
    "    return TF_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jheDGmrRFVVM",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_sorted = TF(freq_dict_1, c=0, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vRR2_cHAFVsS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a single string containing all the words for each corpus\n",
    "\n",
    "word_cloud_list = [ \"\".join( (str(word[0])+\" \" )*int(word[1])) for word in TF_sorted]\n",
    "west_cloud_string = \"\".join(word_cloud_list)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(15, 6)  # size of the figure (the outside box)\n",
    "fig.suptitle(\"Word Cloud Monarchs' Corpus\", fontsize=16)\n",
    "\n",
    "wordcloud_1 = WordCloud(width=1000, height=600,max_font_size=50,  collocations = False, colormap='Oranges_r').generate(west_cloud_string)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud_1, interpolation=\"bilinear\" ,)\n",
    "#plt.title(\"Word Cloud Monarchs' Corpus\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout(  )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RplzYTfAC-V-",
   "metadata": {
    "hidden": true,
    "id": "RplzYTfAC-V-"
   },
   "source": [
    "**Figure 3.2<br>**\n",
    "\n",
    "The figure above shows the word cloud with the most common used words of all the monarchs' corpus. As expected words related to royality as the titles  price, duke and princess are among the most frequent words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TTNHvOWpIUQN",
   "metadata": {
    "hidden": true,
    "id": "TTNHvOWpIUQN"
   },
   "source": [
    "### Word cloud comparison\n",
    "Here,a wordcloud combarison between all nodes wiht an imbreeding score below the T_1  and above T_2 threshold is performed.\n",
    "With the T_1 threshold () all nodes that......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bMzcfRHkFV6k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the nodes names and put in a list\n",
    "node_names = list(G.nodes())\n",
    "#concert inbreeding score to a % value\n",
    "#imbreed_scores = [G.nodes[node]['inbreed_score_NEW']*100 for node in node_names]\n",
    "\n",
    "#create diuctionary with all nodes names and scores\n",
    "my_dict_imbr = dict(zip(node_names, inbreed_scores))\n",
    "# Sort the dictionary by values in descending order\n",
    "sorted_dict_imbr = dict(sorted(my_dict_imbr.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Visualize the 10 persons with higher inbreeding score :\n",
    "first_10_items = dict(list(sorted_dict_imbr.items())[:30])\n",
    "print(\"The 10 perosns with higher inbreeding score are : \",first_10_items)\n",
    "\n",
    "print(\"\\n \\n The imbreading score which have a value different from  are :\", len([i for i in inbreed_scores if i != 0]) , \"froma a total number of node equal to : \" ,len(node_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DLstnrZbPPMS",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes_up = [key for key, value in sorted_dict_imbr.items() if value > threshold_up ]\n",
    "nodes_down = [key for key, value in sorted_dict_imbr.items() if value < threshold_down]\n",
    "\n",
    "# create a corpus with the text of the nodes with high imbreeding score\n",
    "def corpus_from_nodes(nodes_list, stop_words= stop_words, G=G):\n",
    "    list_tokens= []\n",
    "    for node in nodes_list:\n",
    "        # extract tokens attribute from the node\n",
    "        tokens = G.nodes[node][\"tokens\"]\n",
    "        # removw not alphabetic words and stopwords\n",
    "        tokens_NoStopWords = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    #return corpus as a list of tokens\n",
    "        list_tokens += tokens_NoStopWords\n",
    "        corpus = nltk.Text(tokens_NoStopWords)\n",
    "\n",
    "    return list_tokens , corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bzRGMYnjPPj_",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_upper_imbr_nostop , corpus_upper = corpus_from_nodes(nodes_up , stop_words= stop_words, G=G)\n",
    "tokens_lower_imbr_nostop , corpus_lower = corpus_from_nodes(nodes_down , stop_words= stop_words, G=G)\n",
    "print(\"Len tokens upper imbr:\", len(tokens_upper_imbr_nostop))\n",
    "print(\"Len tokens lower imbr:\", len(tokens_lower_imbr_nostop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aq2g-fbEPkrw",
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_disorders = ['Hemophilia','Cystic Fibrosis','Huntington\\'s Disease','Tay-Sachs Disease','Autosomal Recessive Disorders','Mental Retardation',\n",
    "    'Neurofibromatosis','Schizophrenia','Bipolar Disorder','Inherited Neuromuscular Disorders','Down Syndrome','Turner Syndrome',\n",
    "    'Klinefelter Syndrome','Marfan Syndrome','Sickle Cell Anemia','Thalassemia','Spinal Muscular Atrophy','Albinism','Phenylketonuria (PKU)',\n",
    "    'Fragile X Syndrome','Williams Syndrome','Gaucher Disease','Muscular Dystrophy','Cerebral Palsy','Ehlers-Danlos Syndrome','Rett Syndrome',\n",
    "    'Polycystic Kidney Disease','Prader-Willi Syndrome','Cri du Chat Syndrome', \"mad\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EVuNpap1FWCf",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict_upper = nltk.FreqDist(tokens_upper_imbr_nostop)\n",
    "freq_dict_lower = nltk.FreqDist(tokens_lower_imbr_nostop)\n",
    "\n",
    "def TF_TR(freq_dict_1, freq_dict_2, c=1, normalization=True , DIS=1):\n",
    "    # regolarization factor for balancing the different world count\n",
    "    #tokens count east, tokens count west\n",
    "    count_1 = sum(freq_dict_1.values())\n",
    "    count_2 = sum(freq_dict_2.values())\n",
    "\n",
    "    disorder_count = 0\n",
    "    TF_TR = []\n",
    "\n",
    "    if normalization:\n",
    "        for i in freq_dict_1.keys():\n",
    "            if i in freq_dict_2.keys():\n",
    "                if i in genetic_disorders:\n",
    "                    TF_TR.append((i, round( ((freq_dict_1[i]/count_1 )   /(freq_dict_2[i]/count_2 ))*DIS  ,4) ))\n",
    "                    disorder_count +=1\n",
    "                else:\n",
    "                    TF_TR.append((i, round( (freq_dict_1[i]/count_1 )   /(freq_dict_2[i]/count_2 )  ,4) ))\n",
    "\n",
    "            else:\n",
    "                if i in genetic_disorders:\n",
    "                    TF_TR.append((i, round ( ((freq_dict_1[i]/count_1 )/c) *DIS ,4) ))\n",
    "                    disorder_count +=1\n",
    "                else:\n",
    "                    TF_TR.append((i, round ( (freq_dict_1[i]/count_1 )/c ,4) ))\n",
    "    else:\n",
    "        for i in freq_dict_1.keys():\n",
    "                if i in freq_dict_2.keys():\n",
    "                    TF_TR.append((i, round(freq_dict_1[i]/freq_dict_2[i],2) ))\n",
    "                else:\n",
    "                    TF_TR.append((i, round (freq_dict_1[i]/c ,2) ))\n",
    "\n",
    "\n",
    "    TF_TR_sorted = sorted(TF_TR, key=lambda tup: tup[1], reverse=True)\n",
    "    print(\"disorder count:\", disorder_count)\n",
    "    return TF_TR_sorted\n",
    "\n",
    "#calculate TF_TR for the two corpus (upper and lower imbreeding score)\n",
    "\n",
    "TF_TR_upper = TF_TR(freq_dict_upper, freq_dict_lower, c=1, normalization=True)\n",
    "\n",
    "\n",
    "TF_TR_lower = TF_TR(freq_dict_lower, freq_dict_upper, c=1, normalization=True)\n",
    "\n",
    "# create a list and string with the words repeated according to their TF_TR score ( needed for the wordcloud)\n",
    "\n",
    "upper_cloud_list = [ \"\".join( (str(word[0])+\" \" )*int(word[1])) for word in TF_TR_upper]\n",
    "# add a word 20000 times to make it appear in the wordcloud\n",
    "#upper_cloud_list.append(\"UPPERMY \"*20000)\n",
    "\n",
    "\n",
    "upper_cloud_string = \"\".join(upper_cloud_list)#\n",
    "# add a word wit\n",
    "\n",
    "lower_cloud_list = [ \"\".join( (str(word[0])+\" \" )*int(word[1])) for word in TF_TR_lower]\n",
    "lower_cloud_string = \"\".join(lower_cloud_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uTXKh9kRFWI6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word cloud obj\n",
    "wordcloud_upper = WordCloud(width=1000, height=600,max_font_size=50,  collocations = False, colormap= 'Reds').generate(upper_cloud_string)\n",
    "wordcloud_lower = WordCloud(width=1000, height=600,max_font_size=50,  collocations = False, colormap='Greens').generate(lower_cloud_string)\n",
    "\n",
    "#create figure\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(15, 6)  # size of the figure (the outside box)\n",
    "fig.suptitle('WordClouds of the more characteristics words for each cost', fontsize=16)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_upper, interpolation=\"bilinear\" ,)\n",
    "plt.title(f\"Inbreeding score > {threshold_up} \")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_lower, interpolation=\"bilinear\")\n",
    "plt.title(f\"Inbreeding score < {threshold_down} \")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(  )\n",
    "plt.show()\n",
    "\n",
    "#shows the nummber of tokens considerated in each group (could be removed at the end)\n",
    "print( \"The number of tokens inside the group with inbreeding score above (>15) are :\",len(upper_cloud_string))\n",
    "print( \"The number of tokens inside the group with inbreeding score less then (<1) are \",len(lower_cloud_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qfF-dYISW9fs",
   "metadata": {
    "hidden": true,
    "id": "qfF-dYISW9fs"
   },
   "source": [
    "**Figure 3.3** <br>\n",
    "From the wordcloud comparison above it could be seen that no particular  words concerning inbreeding issues were found in the group for high healt risk  from the inbreeding score. As it could be seen in the wordcloud of high inbred individuals not any word  rilevant to helth or inbreeding was found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UDPXBW_0OH-V",
   "metadata": {
    "hidden": true,
    "id": "UDPXBW_0OH-V"
   },
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V3rZmOr0-6n1",
   "metadata": {
    "id": "V3rZmOr0-6n1"
   },
   "source": [
    "Finally, we'll run multiple sentiment analysis with different dictionaries, to see if there is a difference within the texts of inbred and non inbred people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OaEUQ0ldFCPF",
   "metadata": {
    "hidden": true,
    "id": "OaEUQ0ldFCPF"
   },
   "source": [
    "## Sentiment score from the sentiment score analyzer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KYhyJxlcEuC_",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the lists nodes_up and nodes_down that refers to nodes with particularly high inbreeding score helath risk\n",
    "# and low risk were defined in the subsection above Wordcloud comparison\n",
    "\n",
    "# create a list for the two groups for high, low imbreeding extracting their seniment score\n",
    "sentiment_scores_up = [G.nodes[node]['sentiment_score_2'] for node in nodes_up]\n",
    "sentiment_scores_down = [G.nodes[node]['sentiment_score_2'] for node in nodes_down]  #helt score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZDklRoTAEuWz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot comparing imbreed_scores_up against imbreed_scores_down distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=False)\n",
    "\n",
    "# Figure title\n",
    "fig.suptitle('Sentiment analyzer score for high and low inbreeding score', fontsize=16)\n",
    "\n",
    "# Create histograms for each set of scores with density=True for percentage\n",
    "hist_up = ax1.hist(sentiment_scores_up, bins=30, color='red', alpha=0.7,  density=False)\n",
    "hist_down = ax2.hist(sentiment_scores_down, bins=80, color='blue', alpha=0.7,  density=False)\n",
    "\n",
    "# Set x-axis limits for better comparison\n",
    "x_min, x_max = min(min(sentiment_scores_up), min(sentiment_scores_down) -0.5), max(\n",
    "    max(sentiment_scores_up) + 0.5, max(sentiment_scores_down)\n",
    ")\n",
    "ax1.set_xlim(x_min, x_max)\n",
    "ax2.set_xlim(x_min, x_max)\n",
    "\n",
    "# Calculate statistics for each set of scores\n",
    "mean_up = np.mean(sentiment_scores_up)\n",
    "percentile_5_up = np.percentile(sentiment_scores_up, 5)\n",
    "percentile_95_up = np.percentile(sentiment_scores_up, 95)\n",
    "median_up = np.median(sentiment_scores_up)\n",
    "\n",
    "mean_down = np.mean(sentiment_scores_down)\n",
    "percentile_5_down = np.percentile(sentiment_scores_down, 5)\n",
    "percentile_95_down = np.percentile(sentiment_scores_down, 95)\n",
    "median_down = np.median(sentiment_scores_down)\n",
    "\n",
    "# Add legends, labels, and titles to the plot\n",
    "ax1.axvline(mean_up, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_up:.2f}')\n",
    "ax1.axvline(percentile_5_up, color='green', linestyle='dashed', linewidth=2, label=f'5th Percentile: {percentile_5_up:.2f}', alpha=0.5)\n",
    "ax1.axvline(percentile_95_up, color='green', linestyle='dashed', linewidth=2, label=f'95th Percentile: {percentile_95_up:.2f}', alpha=0.5)\n",
    "ax1.axvline(median_up, color='violet', linestyle='-', linewidth=2, label=f'Median: {median_up:.2f}', alpha=0.8)\n",
    "\n",
    "ax2.axvline(mean_down, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_down:.2f}')\n",
    "ax2.axvline(percentile_5_down, color='green', linestyle='dashed', linewidth=2, label=f'5th Percentile: {percentile_5_down:.2f}', alpha=0.5)\n",
    "ax2.axvline(percentile_95_down, color='green', linestyle='dashed', linewidth=2, label=f'95th Percentile: {percentile_95_down:.2f}', alpha=0.5)\n",
    "ax2.axvline(median_down, color=\"violet\", linestyle='-', linewidth=2, label=f'Median: {median_down:.2f}', alpha=0.8)\n",
    "\n",
    "# Add legends, labels, and titles to the plot\n",
    "legend1 = ax1.legend(loc='upper right', shadow=True, fontsize='small')\n",
    "legend1.get_frame().set_facecolor('C0')\n",
    "\n",
    "legend2 = ax2.legend(loc='upper right', shadow=True, fontsize='small')\n",
    "legend2.get_frame().set_facecolor('C1')\n",
    "ax1.legend(loc='upper center', fancybox=True, shadow=True)\n",
    "\n",
    "ax1.set_xlabel('Sentiment score ')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('High inbreeding score group')\n",
    "ax1.legend(loc='upper center', fancybox=True, shadow=True)\n",
    "\n",
    "ax2.set_xlabel('Sentiment score ')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Low inbreeding score group')\n",
    "ax2.legend(loc='upper center', fancybox=True, shadow=True)\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EWRRj_jZ29j5",
   "metadata": {
    "hidden": true,
    "id": "EWRRj_jZ29j5"
   },
   "source": [
    "**Figure 4.1** <br>\n",
    "The figure above shows that the sentiment score analyzer for the High inbreed group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ontpw7_XxoP2",
   "metadata": {
    "id": "ontpw7_XxoP2"
   },
   "source": [
    "## Sentiment score LabMT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8lkbDCfH_Ow8",
   "metadata": {
    "id": "8lkbDCfH_Ow8"
   },
   "source": [
    "We'll test the dictionary used in class for happy/sad topics, to see if we can see a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M5FGLESOrg7I",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the lists nodes_up and nodes_down that refers to nodes with particularly high inbreeding score helath risk\n",
    "# and low risk were defined in the subsection above Wordcloud comparison\n",
    "\n",
    "# create a list for the two groups for high, low imbreeding extracting their seniment score\n",
    "sentiment_scores_up_1 = [G.nodes[node]['sentiment_score_1'] for node in nodes_up]\n",
    "sentiment_scores_down_1 = [G.nodes[node]['sentiment_score_1'] for node in nodes_down]  #helt score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F8x_q-Blx7gl",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=False)\n",
    "\n",
    "# Figure title\n",
    "fig.suptitle('LabMT Sentiment score for high and low inbreeding score', fontsize=16)\n",
    "\n",
    "# Create histograms for each set of scores with density=True for percentage\n",
    "hist_up = ax1.hist(sentiment_scores_up_1, bins=30, color='red', alpha=0.7,  density=False)\n",
    "hist_down = ax2.hist(sentiment_scores_down_1, bins=80, color='blue', alpha=0.7,  density=False)\n",
    "\n",
    "# Set x-axis limits for better comparison\n",
    "#x_min, x_max = min(min(sentiment_scores_up_1), min(sentiment_scores_down_1) -0.5), max(\n",
    " #   max(sentiment_scores_up_1) + 0.5, max(sentiment_scores_down_1)\n",
    "\n",
    "#ax1.set_xlim(x_min, x_max)\n",
    "ax2.set_xlim(4.9, 6.1)\n",
    "ax1.set_xlim(4.9, 6.1)\n",
    "# Calculate statistics for each set of scores\n",
    "mean_up = np.mean(sentiment_scores_up_1)\n",
    "percentile_5_up = np.percentile(sentiment_scores_up_1, 5)\n",
    "percentile_95_up = np.percentile(sentiment_scores_up_1, 95)\n",
    "median_up = np.median(sentiment_scores_up_1)\n",
    "\n",
    "mean_down = np.mean(sentiment_scores_down_1)\n",
    "percentile_5_down = np.percentile(sentiment_scores_down_1, 5)\n",
    "percentile_95_down = np.percentile(sentiment_scores_down_1, 95)\n",
    "median_down = np.median(sentiment_scores_down_1)\n",
    "\n",
    "# Add legends, labels, and titles to the plot\n",
    "ax1.axvline(mean_up, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_up:.2f}')\n",
    "ax1.axvline(percentile_5_up, color='green', linestyle='dashed', linewidth=2, label=f'5th Percentile: {percentile_5_up:.2f}', alpha=0.5)\n",
    "ax1.axvline(percentile_95_up, color='green', linestyle='dashed', linewidth=2, label=f'95th Percentile: {percentile_95_up:.2f}', alpha=0.5)\n",
    "ax1.axvline(median_up, color='violet', linestyle='-', linewidth=2, label=f'Median: {median_up:.2f}', alpha=0.8)\n",
    "\n",
    "ax2.axvline(mean_down, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_down:.2f}')\n",
    "ax2.axvline(percentile_5_down, color='green', linestyle='dashed', linewidth=2, label=f'5th Percentile: {percentile_5_down:.2f}', alpha=0.5)\n",
    "ax2.axvline(percentile_95_down, color='green', linestyle='dashed', linewidth=2, label=f'95th Percentile: {percentile_95_down:.2f}', alpha=0.5)\n",
    "ax2.axvline(median_down, color='violet', linestyle='-', linewidth=2, label=f'Median: {median_down:.2f}', alpha=0.8)\n",
    "\n",
    "# Add legends, labels, and titles to the plot\n",
    "legend1 = ax1.legend(loc='upper right', shadow=True, fontsize='small')\n",
    "legend1.get_frame().set_facecolor('C0')\n",
    "\n",
    "legend2 = ax2.legend(loc='upper right', shadow=True, fontsize='small')\n",
    "legend2.get_frame().set_facecolor('C1')\n",
    "ax1.legend(loc='upper center', fancybox=True, shadow=True)\n",
    "\n",
    "ax1.set_xlabel('LabMT Sentiment score ')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('High inbreeding score group(>15)')\n",
    "ax1.legend( fancybox=True, shadow=True)\n",
    "\n",
    "ax2.set_xlabel('LabMT Sentiment score ')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Low inbreeding score group (<1)')\n",
    "ax2.legend( fancybox=True, shadow=True)\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VNy31gQ5x_OA",
   "metadata": {
    "id": "VNy31gQ5x_OA"
   },
   "source": [
    "**Figure 4.1** <br>\n",
    "The figure above shows comparison of the LabMT distibution for the two groups:high (>15) and low(<1) inbreeding score.\n",
    "As it could be seen both the distibution appear to have a Normal shape, and have also the same exact mean.\n",
    "With this analysis, a difference in the wikitext  sentiment using the was searched for high and low inbreeding persons, however any significant result was found."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
